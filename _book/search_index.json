[
["index.html", "DAM AT1-A 1 Planning", " DAM AT1-A Joshua McCarthy 20/04/2020 1 Planning library(bookdown) library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.0 ✓ purrr 0.3.3 ## ✓ tibble 2.1.3 ✓ dplyr 0.8.5 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## intersect, setdiff, union ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(glmnet) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Loaded glmnet 3.0-2 library(corrplot) ## corrplot 0.84 loaded library(Metrics) Install bookdown and build this page to see full book "],
["understanding.html", "2 Understanding 2.1 Initial Import and Data Prep 2.2 Data Quality 2.3 Initial Understanding 2.4 Monthly Amount 2.5 Target dataset 2.6 Aggregate mean 2.7 Conclusions", " 2 Understanding 2.1 Initial Import and Data Prep # import dataset #list of common NA substitution terms # 0 is not included intentionally pot_nas &lt;- c(&quot;&quot;, &quot; &quot;, &quot; &quot;, &quot;.&quot;, &quot;,&quot;, &quot;NaN&quot;, &quot;NAN&quot;, &quot;nan&quot;, &quot;NA&quot;, &quot;na&quot;, &quot;N/A&quot;, &quot;n/a&quot;) # To make checking for missing values, including the read_csv(,na) argument allows us to assign all missing values defined in pot_nas as NA transactions &lt;- read_csv(&quot;./core_data/transactions.csv&quot;, na = pot_nas) ## Parsed with column specification: ## cols( ## date = col_character(), ## customer_id = col_character(), ## industry = col_double(), ## location = col_double(), ## monthly_amount = col_double() ## ) # check names match dictionary names(transactions) ## [1] &quot;date&quot; &quot;customer_id&quot; &quot;industry&quot; &quot;location&quot; ## [5] &quot;monthly_amount&quot; # rename date to trdate due to potential issues with R date function tname &lt;- names(transactions) tname[1] &lt;- &quot;trdate&quot; names(transactions) &lt;- tname #check for NA values if (nrow(transactions) == nrow(na.omit(transactions))){ print(&quot;no missing values&quot;) } else { print(&quot;missing values present&quot;) } ## [1] &quot;no missing values&quot; #data does not appear to have any missing values # check data structure str(transactions) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 94248 obs. of 5 variables: ## $ trdate : chr &quot;1/1/13&quot; &quot;1/2/13&quot; &quot;1/3/13&quot; &quot;1/4/13&quot; ... ## $ customer_id : chr &quot;70efdf2ec9b086079795c442636b55fb&quot; &quot;70efdf2ec9b086079795c442636b55fb&quot; &quot;70efdf2ec9b086079795c442636b55fb&quot; &quot;70efdf2ec9b086079795c442636b55fb&quot; ... ## $ industry : num 8 8 8 8 8 8 8 8 8 8 ... ## $ location : num 9 9 9 9 9 9 9 9 9 9 ... ## $ monthly_amount: num 753851 651548 1138769 659739 770675 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. date = col_character(), ## .. customer_id = col_character(), ## .. industry = col_double(), ## .. location = col_double(), ## .. monthly_amount = col_double() ## .. ) # date variable not returned in date format transactions$trdate &lt;- as.Date(transactions$trdate, format = &quot;%d/%m/%y&quot;) # update industry, customer and location as they are all factors transactions$customer_id &lt;- as.factor(transactions$customer_id) transactions$industry &lt;- as.factor(transactions$industry) transactions$location &lt;- as.factor(transactions$location) # double check formatting str(transactions) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 94248 obs. of 5 variables: ## $ trdate : Date, format: &quot;2013-01-01&quot; &quot;2013-02-01&quot; ... ## $ customer_id : Factor w/ 4464 levels &quot;000a91f3e374e6147d58ed1814247508&quot;,..: 1970 1970 1970 1970 1970 1970 1970 1970 1970 1970 ... ## $ industry : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ location : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 9 9 9 9 9 9 9 9 9 9 ... ## $ monthly_amount: num 753851 651548 1138769 659739 770675 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. date = col_character(), ## .. customer_id = col_character(), ## .. industry = col_double(), ## .. location = col_double(), ## .. monthly_amount = col_double() ## .. ) summary(transactions) ## trdate customer_id industry ## Min. :2013-01-01 0023a1e3447fdb31836536cc903f1310: 47 1 :44901 ## 1st Qu.:2014-05-01 006c64491cb8acf2092ce0e0341797fe: 47 2 :22043 ## Median :2015-06-01 0079e3e6d496ad07cee7fd63d3d7c9b2: 47 3 : 8336 ## Mean :2015-03-26 00989c20ff1386dc386d8124ebcba1a5: 47 4 : 5896 ## 3rd Qu.:2016-03-01 00bbd6beace7365b27a913db75fddce9: 47 7 : 3912 ## Max. :2016-11-01 01882513d5fa7c329e940dda99b12147: 47 5 : 3341 ## (Other) :93966 (Other): 5819 ## location monthly_amount ## 2 :21410 Min. : 0 ## 1 :17660 1st Qu.: 95323 ## 5 : 9703 Median : 179399 ## 4 : 9351 Mean : 395397 ## 6 : 7446 3rd Qu.: 375439 ## 7 : 7317 Max. :100000000 ## (Other):21361 The summary shows; * Date range from Jan 2013 to Dec 2016 * 4464 unique customers * All 10 locations and industries are represtened * There is a disproportionate number of transactions in industires 1 &amp; 2 * There is a disproportionate number of transactions in locations 1 &amp; 2 * The monthly transaction amounts range from 0 - 100M ** The mean is 395396 and median is 179399, low considering the range 2.2 Data Quality 2.3 Initial Understanding 2.3.1 Customers # Do all customer have equal numbers of transactions? ## counts transactions by customer_id, formats as a table number_of_transactions_per_customer &lt;- as.data.frame(table(transactions$customer_id)) summary(number_of_transactions_per_customer) ## Var1 Freq ## 000a91f3e374e6147d58ed1814247508: 1 Min. : 1.00 ## 0014fcb3db4c8459d26309b177005b10: 1 1st Qu.: 8.00 ## 0023a1e3447fdb31836536cc903f1310: 1 Median :18.00 ## 0023daa5a44ef6fbe42379f24e00254c: 1 Mean :21.11 ## 0029f088c57ad3b6ec589f9ba4f7a057: 1 3rd Qu.:33.00 ## 003af5a042e00ac9b489153a81d676ca: 1 Max. :47.00 ## (Other) :4458 # Answer - No # Do any customers have duplicate transactions on the same date? ## creates a dataframe of the trdate and customer_id columns duplicate_check &lt;- transactions %&gt;% select(trdate, customer_id) ## appends a column to the dataframe if there is a duplicate transaction duplicate_check$dup &lt;- transactions %&gt;% select(trdate, customer_id) %&gt;% duplicated() ## creates a table of rows that are duplicates dup_check_sum &lt;- duplicate_check %&gt;% filter(duplicate_check$dup == &quot;TRUE&quot;) ## if there are no entries, therefore no duplicates in the new table prints &quot;no duplicates if (nrow(dup_check_sum) == 0){ print (&quot;No duplicates&quot;) } else{ print(&quot;Yes duplicates&quot;) } ## [1] &quot;No duplicates&quot; # Answer - No # Do any customers operate across industries? ## selects the columns customer_id and industry and then writes unique entries to dataframe industry_check &lt;- transactions %&gt;% select(industry, customer_id) %&gt;% distinct() ## selects only the customer id and checks for duplicates industry_check$multi_industry &lt;- industry_check %&gt;% select(customer_id) %&gt;% duplicated() ## if the customer id is duplicated, that means that it appeared in more than one industry ## same as duplicate check for dates industry_checksum &lt;- industry_check %&gt;% filter(multi_industry == &quot;TRUE&quot;) if (nrow(industry_checksum) == 0){ print (&quot;No duplicate&quot;) } else{ print(&quot;Yes duplicates&quot;) } ## [1] &quot;No duplicate&quot; # Answer - No # Do any customers operate across locations? ## same process as industry but for location location_check &lt;- transactions %&gt;% select(location, customer_id) %&gt;% distinct() location_check$multi_location &lt;- location_check %&gt;% select(customer_id) %&gt;% duplicated() location_checksum&lt;- location_check %&gt;% filter(multi_location == &quot;TRUE&quot;) if (nrow(location_checksum) == 0){ print (&quot;No&quot;) } else{ print(&quot;Yes&quot;) } ## [1] &quot;No&quot; # Answer - No 2.3.2 Industry # Number of transactons per industry ## using only customer_id and industry, creates a bar plot, counting how many times each industry appeared transactions %&gt;% select(customer_id, industry) %&gt;% ggplot(aes(x = industry)) + geom_bar() # Industries 1 &amp; 2 have a much larger number of transactions, similarly industries 6 &amp; 10 appear quite low ## create a summary of transactions per industry in a table number_of_transactions_per_industry &lt;- as.data.frame(table(transactions$industry)) ## give the columns readable names names(number_of_transactions_per_industry) &lt;- c(&quot;industry&quot;, &quot;count&quot;) number_of_transactions_per_industry ## industry count ## 1 1 44901 ## 2 2 22043 ## 3 3 8336 ## 4 4 5896 ## 5 5 3341 ## 6 6 195 ## 7 7 3912 ## 8 8 2815 ## 9 9 2090 ## 10 10 719 # calculates mean of transactions per industry ntpi_mean &lt;- mean(number_of_transactions_per_industry$count) ## calculates standard deviation of transactions per industry ntpi_sd &lt;- sd(number_of_transactions_per_industry$count) ## calculates standard deviations from mean for each count number_of_transactions_per_industry$SDs &lt;- (number_of_transactions_per_industry$count - ntpi_mean)/ntpi_sd number_of_transactions_per_industry ## industry count SDs ## 1 1 44901 2.53884748 ## 2 2 22043 0.90301908 ## 3 3 8336 -0.07791976 ## 4 4 5896 -0.25253790 ## 5 5 3341 -0.43538599 ## 6 6 195 -0.66052888 ## 7 7 3912 -0.39452248 ## 8 8 2815 -0.47302908 ## 9 9 2090 -0.52491356 ## 10 10 719 -0.62302892 # industry 1 is 2.5 standard deviations from the mean however industry 2 is within 1 standard deviation # number of customer per industry ## creates a bar plot, counting how many times each industry appeared transactions %&gt;% ## selects only unique customer id&#39;s per industry so each customer is only counted once distinct(customer_id, industry) %&gt;% ggplot(aes(x = industry)) + geom_bar() # a similar trend appears in the number of customers, likely as each customer can have a maximum of 47 transactions ## creates a table summarising each customer into a single entry, keeping other information number_of_customers_per_industry &lt;- group_by(transactions, customer_id, industry) %&gt;% summarise(mean_amount = mean(monthly_amount)) ## organises counts into a dataframe number_of_customers_per_industry &lt;- as.data.frame(table(number_of_customers_per_industry$industry)) ## readable column headers names(number_of_customers_per_industry) &lt;- c(&quot;industry&quot;, &quot;count&quot;) number_of_customers_per_industry ## industry count ## 1 1 1953 ## 2 2 1364 ## 3 3 399 ## 4 4 199 ## 5 5 127 ## 6 6 7 ## 7 7 184 ## 8 8 109 ## 9 9 90 ## 10 10 32 ## deviations from mean count ncpi_mean &lt;- mean(number_of_customers_per_industry$count) ncpi_sd &lt;- sd(number_of_customers_per_industry$count) number_of_customers_per_industry$SDs &lt;- (number_of_customers_per_industry$count - ncpi_mean)/ncpi_sd number_of_customers_per_industry ## industry count SDs ## 1 1 1953 2.27363548 ## 2 2 1364 1.38476564 ## 3 3 399 -0.07153214 ## 4 4 199 -0.37335551 ## 5 5 127 -0.48201193 ## 6 6 7 -0.66310595 ## 7 7 184 -0.39599227 ## 8 8 109 -0.50917603 ## 9 9 90 -0.53784925 ## 10 10 32 -0.62537803 # industry 1 has 2.2 standard deviation higher customer count, this skews the sd count for other industries # industry 7 only has a total of 7 customers ## subset data for industry 6 industry_6 &lt;- transactions %&gt;% filter(industry == 6) summary(industry_6) ## trdate customer_id industry ## Min. :2013-01-01 6c530aae768250b8d9c3c908a13ee287:47 6 :195 ## 1st Qu.:2013-10-01 d278df4919453195d221030324127a0e:47 1 : 0 ## Median :2014-07-01 23755432da68528f115c9633c0d7834f:28 2 : 0 ## Mean :2014-08-30 ecf4afbea9a7f57c4c6da70593361d67:22 3 : 0 ## 3rd Qu.:2015-07-01 8196e8d0f9ee7dfdfc7e11dbbfa30d77:21 4 : 0 ## Max. :2016-11-01 a2a7902052d85a18a7b564d8872f1ff6:19 5 : 0 ## (Other) :11 (Other): 0 ## location monthly_amount ## 1 :195 Min. : 0 ## 2 : 0 1st Qu.: 680283 ## 3 : 0 Median : 21588432 ## 4 : 0 Mean : 27063231 ## 5 : 0 3rd Qu.: 47028767 ## 6 : 0 Max. :100000000 ## (Other): 0 nrow(industry_6) ## [1] 195 # 195 total entries for industry 6 # all 7 customers in industry 6 operate in location 1 # the range is still 0-100M meaning both values exist in industry 6 #try removing the industry subset and checking table summary ## subset data without industry 6 industry_no_6 &lt;- transactions %&gt;% filter(industry != 6) summary(industry_no_6) ## trdate customer_id industry ## Min. :2013-01-01 0023a1e3447fdb31836536cc903f1310: 47 1 :44901 ## 1st Qu.:2014-05-01 006c64491cb8acf2092ce0e0341797fe: 47 2 :22043 ## Median :2015-06-01 0079e3e6d496ad07cee7fd63d3d7c9b2: 47 3 : 8336 ## Mean :2015-03-27 00989c20ff1386dc386d8124ebcba1a5: 47 4 : 5896 ## 3rd Qu.:2016-03-01 00bbd6beace7365b27a913db75fddce9: 47 7 : 3912 ## Max. :2016-11-01 01882513d5fa7c329e940dda99b12147: 47 5 : 3341 ## (Other) :93771 (Other): 5624 ## location monthly_amount ## 2 :21410 Min. : 45986 ## 1 :17465 1st Qu.: 95230 ## 5 : 9703 Median : 179018 ## 4 : 9351 Mean : 340107 ## 6 : 7446 3rd Qu.: 373432 ## 7 : 7317 Max. :64935696 ## (Other):21361 # the lowest value is now $45k and highest is $64M # Meaning industry 6 contains both the lowest and highest monthly_amounts in the dataset # industry 6&#39;s data looks questionable 2.3.3 Industry 6 # if all zero values in the dataset are in industry 6, are zero values common? industry_6 %&gt;% filter(monthly_amount == 0) ## # A tibble: 1 x 5 ## trdate customer_id industry location monthly_amount ## &lt;date&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2015-03-01 23755432da68528f115c9633c0d7834f 6 1 0 #only one monthly amount of 0, and the next lowest is 45k in the dataset indicates its inclusion is likely and error # remove this value from industry_6 subset and main dataset industry_6 &lt;- industry_6 %&gt;% filter(monthly_amount != 0) transactions &lt;- transactions %&gt;% filter(monthly_amount != 0) ## plotting monlthy amount against date, colouring the plot by customer_id ggplot(industry_6, aes(x = trdate, y = monthly_amount, color = customer_id)) + #changes the opacity of each point so overlapping points are easier to differentiate geom_point(alpha = 0.5) + facet_wrap(industry_6$customer_id) #mean of each customer&#39;s monthly payments in industry_6 i6_customer_summary &lt;- industry_6 %&gt;% group_by(customer_id) %&gt;% summarise(mean_amount = mean(monthly_amount)) i6_customer_summary ## # A tibble: 7 x 2 ## customer_id mean_amount ## &lt;fct&gt; &lt;dbl&gt; ## 1 23755432da68528f115c9633c0d7834f 54404455. ## 2 6c530aae768250b8d9c3c908a13ee287 494993. ## 3 8196e8d0f9ee7dfdfc7e11dbbfa30d77 18846950. ## 4 a2a7902052d85a18a7b564d8872f1ff6 1680022. ## 5 bd853b475d59821e100d3d24303d7747 15806921. ## 6 d278df4919453195d221030324127a0e 35334792. ## 7 ecf4afbea9a7f57c4c6da70593361d67 69219425. # it appears the 100M ammount is correct from the plot if it belongs to customer ecf industry_6 %&gt;% filter(monthly_amount == 100000000) ## # A tibble: 1 x 5 ## trdate customer_id industry location monthly_amount ## &lt;date&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2016-10-01 ecf4afbea9a7f57c4c6da70593361d67 6 1 100000000 # it does, it will remain in the dataset # check the distribution of the main dataset with and without industry 6 # with industry 6 (the large spread indicated a need for more bins) ggplot(transactions, aes(x = monthly_amount)) + geom_histogram(bins = 1000) #the data is heavily skewed industry 6&#39;s high amounts for customer 6c530aae768250b8d9c3c908a13ee287 appear to be outliers # without industry 6 ## remake industry_no_6 subset due to removal of 0 transactions industry_no_6 &lt;- transactions %&gt;% filter(industry != 6) summary(industry_no_6) ## trdate customer_id industry ## Min. :2013-01-01 0023a1e3447fdb31836536cc903f1310: 47 1 :44901 ## 1st Qu.:2014-05-01 006c64491cb8acf2092ce0e0341797fe: 47 2 :22043 ## Median :2015-06-01 0079e3e6d496ad07cee7fd63d3d7c9b2: 47 3 : 8336 ## Mean :2015-03-27 00989c20ff1386dc386d8124ebcba1a5: 47 4 : 5896 ## 3rd Qu.:2016-03-01 00bbd6beace7365b27a913db75fddce9: 47 7 : 3912 ## Max. :2016-11-01 01882513d5fa7c329e940dda99b12147: 47 5 : 3341 ## (Other) :93771 (Other): 5624 ## location monthly_amount ## 2 :21410 Min. : 45986 ## 1 :17465 1st Qu.: 95230 ## 5 : 9703 Median : 179018 ## 4 : 9351 Mean : 340107 ## 6 : 7446 3rd Qu.: 373432 ## 7 : 7317 Max. :64935696 ## (Other):21361 ggplot(industry_no_6, aes(x = monthly_amount)) + geom_histogram(bins = 1000) # the data is still quite skewed this will need to be addressed later # the plots for the customer in industry 6 except 6c530aae768250b8d9c3c908a13ee287 and a2a7902052d85a18a7b564d8872f1ff6 appear legitimate at this scale a closer inspection of those two customers is needed to reduce the issues with scaling # customer 6c530aae768250b8d9c3c908a13ee287 industry_6 %&gt;% filter(customer_id == &quot;6c530aae768250b8d9c3c908a13ee287&quot;) %&gt;% ggplot(aes(x= trdate, y = monthly_amount)) + geom_point(alpha = 0.5) # customer a2a7902052d85a18a7b564d8872f1ff6 industry_6 %&gt;% filter(customer_id == &quot;a2a7902052d85a18a7b564d8872f1ff6&quot;) %&gt;% ggplot(aes(x= trdate, y = monthly_amount)) + geom_point(alpha = 0.5) # at their own scales both customers transactions do not look suspicious, though still quite low 2.3.4 Industry 10 # A look into industry 10 industry_10 &lt;- transactions %&gt;% filter(industry == 10) summary(industry_10) ## trdate customer_id industry ## Min. :2013-01-01 0079e3e6d496ad07cee7fd63d3d7c9b2: 47 10 :719 ## 1st Qu.:2013-10-01 6410bb923bcf940b7c57331f7b7db3c6: 47 1 : 0 ## Median :2014-11-01 82aa4b0af34c2313a562076992e50aa3: 47 2 : 0 ## Mean :2014-10-27 99701e768d9a09b314e43a1d9e3e9dfa: 47 3 : 0 ## 3rd Qu.:2015-12-01 9a0f86604fa1dc1686a0cad86a808a5c: 47 4 : 0 ## Max. :2016-11-01 d714d2c5a796d5814c565d78dd16188d: 47 5 : 0 ## (Other) :437 (Other): 0 ## location monthly_amount ## 5 :315 Min. : 63982 ## 2 :167 1st Qu.: 96779 ## 8 : 94 Median : 154789 ## 7 : 61 Mean : 2926552 ## 4 : 29 3rd Qu.: 246281 ## 9 : 21 Max. :64935696 ## (Other): 32 ggplot(industry_10, aes(x = trdate, y = monthly_amount)) + geom_point(alpha = 0.5) + facet_wrap(industry_10$location) # the number of transactions by location within industry 10 appear quite inconsistent industry_10_locations &lt;- as.data.frame(table(industry_10$location)) ## give the columns readable names names(industry_10_locations) &lt;- c(&quot;location&quot;, &quot;count&quot;) industry_10_locations ## location count ## 1 1 16 ## 2 2 167 ## 3 3 12 ## 4 4 29 ## 5 5 315 ## 6 6 0 ## 7 7 61 ## 8 8 94 ## 9 9 21 ## 10 10 4 # industry 10 is missing location 6 # is it possible industry 6 in location 1 corresponds to industry 10 location 6? # industry 10 location 10 only has 4 entires ggplot(filter(industry_10, location == 10), aes(x = trdate, y = monthly_amount)) + geom_point(alpha = 0.5) # the values are low but do not appear illigitimate # industry 10 location 8 has a large difference between 2 sets of transactions industry_10 %&gt;% filter(location == 8) %&gt;% ggplot(aes(x = trdate, y = monthly_amount, color = customer_id)) + geom_point() ## only two customers in industry 10 location 8 i10_l8 &lt;-industry_10 %&gt;% filter(location == 8) # customer d714d2c5a796d5814c565d78dd16188d i10_l8 %&gt;% filter(customer_id == &quot;d714d2c5a796d5814c565d78dd16188d&quot;) %&gt;% ggplot(aes(x = trdate, y = monthly_amount)) + geom_point() # transactions are very cyclic and high # customer db8e1af0cb3aca1ae2d0018624204529 i10_l8 %&gt;% filter(customer_id == &quot;db8e1af0cb3aca1ae2d0018624204529&quot;) %&gt;% ggplot(aes(x = trdate, y = monthly_amount)) + geom_point() # transactions are quite random # Number of transactons per location ## using only customer_id and location, creates a bar plot, counting how many times each location appeared transactions %&gt;% select(customer_id, location) %&gt;% ggplot(aes(x = location)) + geom_bar() # Locations 1 and 2 mirroring industries 1 and 2 have a higher representation, however no single location has such an obviously small number of represetnation as indstury 6 or 10 ## create a summary of transactions per location in a table number_of_transactions_per_location &lt;- as.data.frame(table(transactions$location)) ## give the columns readable names names(number_of_transactions_per_location) &lt;- c(&quot;location&quot;, &quot;count&quot;) number_of_transactions_per_location ## location count ## 1 1 17659 ## 2 2 21410 ## 3 3 6740 ## 4 4 9351 ## 5 5 9703 ## 6 6 7446 ## 7 7 7317 ## 8 8 3478 ## 9 9 4629 ## 10 10 6514 # calculates mean of transactions per location ntpl_mean &lt;- mean(number_of_transactions_per_location$count) ## calculates standard deviation of transactions per location ntpl_sd &lt;- sd(number_of_transactions_per_location$count) ## calculates standard deviations from mean for each count number_of_transactions_per_location$SDs &lt;- (number_of_transactions_per_location$count - ntpl_mean)/ntpl_sd number_of_transactions_per_location ## location count SDs ## 1 1 17659 1.44086555 ## 2 2 21410 2.09722817 ## 3 3 6740 -0.46977785 ## 4 4 9351 -0.01289627 ## 5 5 9703 0.04869787 ## 6 6 7446 -0.34623959 ## 7 7 7317 -0.36881245 ## 8 8 3478 -1.04057360 ## 9 9 4629 -0.83916774 ## 10 10 6514 -0.50932409 # location 2 is ~2 standard deviations outside of the mean # number of customer per location ## creates a bar plot, counting how many times each location appeared transactions %&gt;% ## selects only unique customer id&#39;s per location so each customer is only counted once distinct(customer_id, location) %&gt;% ggplot(aes(x = location)) + geom_bar() # a similar trend appears in the number of customers, likely as each customer can have a maximum of 47 transactions # Further investigation into location would be challenging 2.4 Monthly Amount # coming back to distribution ggplot(transactions, aes(x = monthly_amount)) + geom_histogram(bins = 20) + geom_density() # the transactions are focused in a small range #create a subset within 1sd of the mean tmean &lt;- mean(transactions$monthly_amount) tsd &lt;- sd(transactions$monthly_amount) transactions_1sd &lt;- transactions %&gt;% filter(monthly_amount &gt;= tmean - tsd &amp; monthly_amount &lt;= tmean + tsd) range(transactions_1sd$monthly_amount) ## [1] 45986.23 2371479.06 ggplot(transactions_1sd, aes(x = monthly_amount)) + geom_histogram(aes(y=..density..),bins = 30) + geom_density() density(transactions_1sd$monthly_amount) ## ## Call: ## density.default(x = transactions_1sd$monthly_amount) ## ## Data: transactions_1sd$monthly_amount (93495 obs.); Bandwidth &#39;bw&#39; = 1.856e+04 ## ## x y ## Min. : -9698 Min. :1.900e-11 ## 1st Qu.: 599517 1st Qu.:1.911e-08 ## Median :1208733 Median :6.353e-08 ## Mean :1208733 Mean :4.099e-07 ## 3rd Qu.:1817948 3rd Qu.:3.039e-07 ## Max. :2427163 Max. :5.546e-06 which.max(density(transactions_1sd$monthly_amount, n = 2056)$y) ## [1] 78 tr_densityhigh &lt;- density(transactions_1sd$monthly_amount, n = 2056)$x[which.max(density(transactions_1sd$monthly_amount, n = 2056)$y)] transactions_focus &lt;- transactions %&gt;% filter(monthly_amount &gt;= tr_densityhigh - (tsd/75) &amp; monthly_amount &lt;= tr_densityhigh + tsd/75) ggplot(transactions_focus, aes(x = monthly_amount)) + geom_histogram(aes(y=..density..),bins = 30) + geom_density() range(transactions_focus$monthly_amount) ## [1] 58896.45 107982.14 # subset down again by sd tmean2 &lt;- mean(transactions_1sd$monthly_amount) tsd2 &lt;- sd(transactions_1sd$monthly_amount) transactions_1sd2 &lt;- transactions_1sd %&gt;% filter(monthly_amount &gt;= tmean2 - tsd2 &amp; monthly_amount &lt;= tmean2 + tsd2) ggplot(transactions_1sd2, aes(x = monthly_amount)) + geom_histogram(aes(y=..density..),bins = 30) + geom_density() # transaction distributions broken down by location and industry ggplot(transactions, aes(x= trdate, y = monthly_amount)) + geom_point() + facet_grid(rows = vars(transactions$industry), cols = vars(transactions$location)) # we can see that industry 6 location 1 and industry 8 location 10 have exceptionally high values # removing those two transactions_rm &lt;- transactions %&gt;% filter(industry != 6, location != 10 &amp; location != 8) ggplot(transactions_rm, aes(x= trdate, y = monthly_amount)) + geom_point() + facet_grid(rows = vars(transactions_rm$industry), cols = vars(transactions_rm$location)) ggplot(transactions, aes(x= trdate, y = monthly_amount)) + geom_point() + facet_grid(rows = vars(transactions$industry), cols = vars(transactions$location), scales = &quot;free&quot;) ggplot(transactions, aes(x = monthly_amount)) + geom_histogram(bins = 25) + facet_grid(rows = vars(transactions$industry), cols = vars(transactions$location), scales = &quot;free&quot;) # a slight limitation with facet_grid, attempt to show density plots of each individually #Import new function for managing large plots # Multiple plot function # # ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects) # - cols: Number of columns in layout # - layout: A matrix specifying the layout. If present, &#39;cols&#39; is ignored. # # If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE), # then plot 1 will go in the upper left, 2 will go in the upper right, and # 3 will go all the way across the bottom. # multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } # we will the custom &#39;multiplot&#39; function to make our plots # modified code from CANVAS Announcement #initialise dataframes plotlist_loc_ind &lt;- data.frame() combo_obs &lt;- data.frame() combo_row &lt;- data.frame() #create list of locations and industries industries = unique(transactions$industry) locations = unique(transactions$location) # work through groups in sequence for (ind in industries) { for (loc in locations) { # create a name for each group temp_name &lt;- paste(&quot;industry&quot;, ind, &quot;location&quot;, loc, sep = &quot;_&quot;) # create a subset of the data for each unique group temp = transactions[transactions$industry == ind &amp; transactions$location == loc, ] # create a row of the name and the number of observations in the group combo_row &lt;- cbind(temp_name, nrow(temp)) # add row to list, outputting list of all groups and observations per group combo_obs &lt;- rbind(combo_obs, combo_row) # rename the temp data frame to the name of the group assign(temp_name, temp) # create a vector of all group dataframe names row &lt;- cbind(temp_name) plotlist_loc_ind &lt;- rbind(plotlist_loc_ind, row) } } # transform into a list plotlist_loc_ind &lt;- as.list(as.character(plotlist_loc_ind$temp_name)) #create a list of plots for all groupings (caution large file) plotlist_loc_ind &lt;- lapply(plotlist_loc_ind, function(plotlist_loc_ind){ ggplot(eval(as.symbol(paste(plotlist_loc_ind))), aes(x = monthly_amount)) + geom_density() + theme_void() + labs (title = paste(plotlist_loc_ind)) }) #uses multiplot function to plot all listed plots multiplot(plotlist = plotlist_loc_ind, cols = 3) # not very ordered but the distributions of each group can be visualised # additonally the function created a list of how many observations per grouping str ## function (object, ...) ## UseMethod(&quot;str&quot;) ## &lt;bytecode: 0x55ff2c0bc5b0&gt; ## &lt;environment: namespace:utils&gt; names(combo_obs) &lt;- c(&quot;group&quot;, &quot;obs&quot;) combo_obs$obs &lt;- as.character(combo_obs$obs) combo_obs$obs &lt;- as.numeric(combo_obs$obs) combo_obs ## group obs ## 1 industry_8_location_9 627 ## 2 industry_8_location_4 248 ## 3 industry_8_location_2 432 ## 4 industry_8_location_5 438 ## 5 industry_8_location_10 200 ## 6 industry_8_location_6 190 ## 7 industry_8_location_1 148 ## 8 industry_8_location_3 132 ## 9 industry_8_location_8 260 ## 10 industry_8_location_7 140 ## 11 industry_1_location_9 1528 ## 12 industry_1_location_4 5879 ## 13 industry_1_location_2 8939 ## 14 industry_1_location_5 6093 ## 15 industry_1_location_10 4136 ## 16 industry_1_location_6 5578 ## 17 industry_1_location_1 4601 ## 18 industry_1_location_3 3441 ## 19 industry_1_location_8 1429 ## 20 industry_1_location_7 3277 ## 21 industry_9_location_9 38 ## 22 industry_9_location_4 85 ## 23 industry_9_location_2 1271 ## 24 industry_9_location_5 25 ## 25 industry_9_location_10 22 ## 26 industry_9_location_6 54 ## 27 industry_9_location_1 296 ## 28 industry_9_location_3 183 ## 29 industry_9_location_8 0 ## 30 industry_9_location_7 116 ## 31 industry_10_location_9 21 ## 32 industry_10_location_4 29 ## 33 industry_10_location_2 167 ## 34 industry_10_location_5 315 ## 35 industry_10_location_10 4 ## 36 industry_10_location_6 0 ## 37 industry_10_location_1 16 ## 38 industry_10_location_3 12 ## 39 industry_10_location_8 94 ## 40 industry_10_location_7 61 ## 41 industry_2_location_9 1046 ## 42 industry_2_location_4 1251 ## 43 industry_2_location_2 5940 ## 44 industry_2_location_5 805 ## 45 industry_2_location_10 1160 ## 46 industry_2_location_6 710 ## 47 industry_2_location_1 7046 ## 48 industry_2_location_3 1700 ## 49 industry_2_location_8 435 ## 50 industry_2_location_7 1950 ## 51 industry_5_location_9 393 ## 52 industry_5_location_4 501 ## 53 industry_5_location_2 462 ## 54 industry_5_location_5 131 ## 55 industry_5_location_10 95 ## 56 industry_5_location_6 437 ## 57 industry_5_location_1 676 ## 58 industry_5_location_3 241 ## 59 industry_5_location_8 272 ## 60 industry_5_location_7 133 ## 61 industry_4_location_9 483 ## 62 industry_4_location_4 500 ## 63 industry_4_location_2 809 ## 64 industry_4_location_5 1081 ## 65 industry_4_location_10 472 ## 66 industry_4_location_6 88 ## 67 industry_4_location_1 833 ## 68 industry_4_location_3 241 ## 69 industry_4_location_8 701 ## 70 industry_4_location_7 688 ## 71 industry_6_location_9 0 ## 72 industry_6_location_4 0 ## 73 industry_6_location_2 0 ## 74 industry_6_location_5 0 ## 75 industry_6_location_10 0 ## 76 industry_6_location_6 0 ## 77 industry_6_location_1 194 ## 78 industry_6_location_3 0 ## 79 industry_6_location_8 0 ## 80 industry_6_location_7 0 ## 81 industry_7_location_9 265 ## 82 industry_7_location_4 255 ## 83 industry_7_location_2 737 ## 84 industry_7_location_5 564 ## 85 industry_7_location_10 201 ## 86 industry_7_location_6 182 ## 87 industry_7_location_1 860 ## 88 industry_7_location_3 233 ## 89 industry_7_location_8 153 ## 90 industry_7_location_7 462 ## 91 industry_3_location_9 228 ## 92 industry_3_location_4 603 ## 93 industry_3_location_2 2653 ## 94 industry_3_location_5 251 ## 95 industry_3_location_10 224 ## 96 industry_3_location_6 207 ## 97 industry_3_location_1 2989 ## 98 industry_3_location_3 557 ## 99 industry_3_location_8 134 ## 100 industry_3_location_7 490 combo_obs %&gt;% filter(obs &lt;= 20) ## group obs ## 1 industry_9_location_8 0 ## 2 industry_10_location_10 4 ## 3 industry_10_location_6 0 ## 4 industry_10_location_1 16 ## 5 industry_10_location_3 12 ## 6 industry_6_location_9 0 ## 7 industry_6_location_4 0 ## 8 industry_6_location_2 0 ## 9 industry_6_location_5 0 ## 10 industry_6_location_10 0 ## 11 industry_6_location_6 0 ## 12 industry_6_location_3 0 ## 13 industry_6_location_8 0 ## 14 industry_6_location_7 0 # We also pickup the previously unnoticed unreprestented group of industry 9 in location 8 2.5 Target dataset # The brief specifies to train on industry 1, location 1 taking a closer look a the dataset i1_l1 &lt;- transactions %&gt;% filter(industry ==1, location == 1) summary(i1_l1) ## trdate customer_id industry ## Min. :2013-01-01 0397758f8990c1b41b81b43ac389ab9f: 47 1 :4601 ## 1st Qu.:2014-04-01 0a93091da5efb0d9d5649e7f6b2ad9d7: 47 2 : 0 ## Median :2015-04-01 11953163dd7fb12669b41a48f78a29b6: 47 3 : 0 ## Mean :2015-02-20 16a4d8f9442ec82c5d390079cad8f194: 47 4 : 0 ## 3rd Qu.:2016-02-01 17446a8ae7dbf7e2c2535ba49340b4b9: 47 5 : 0 ## Max. :2016-11-01 25daeb9b3072e9c53f66a2196a92a011: 47 6 : 0 ## (Other) :4319 (Other): 0 ## location monthly_amount ## 1 :4601 Min. : 63716 ## 2 : 0 1st Qu.: 74763 ## 3 : 0 Median : 100240 ## 4 : 0 Mean : 168998 ## 5 : 0 3rd Qu.: 168299 ## 6 : 0 Max. :1509922 ## (Other): 0 ggplot(i1_l1, aes(x = monthly_amount)) + geom_density() # the data is again heavily skewed ggplot(i1_l1, aes(x = trdate, y = monthly_amount)) + geom_point(alpha = 0.1) 2.6 Aggregate mean # The brief also specifies to use an aggregated mean dataset transactions_agg &lt;- aggregate(.~trdate + location + industry, transactions[-2], FUN = mean) transactions_agg %&gt;% filter(industry == 1 &amp; location == 1) %&gt;% ggplot(aes(x = monthly_amount)) + geom_density() #this helps with the distribution but is it actual transactions_agg %&gt;% filter(industry == 1 &amp; location == 1) %&gt;% ggplot(aes(x = trdate, y = monthly_amount)) + geom_point() + geom_line() # Using a similar method to visualise the distributions of the aggregated groupings plotlist_loc_ind_agg &lt;- data.frame() combo_obs &lt;- data.frame() combo_row &lt;- data.frame() industries = unique(transactions_agg$industry) locations = unique(transactions_agg$location) for (ind in industries) { for (loc in locations) { temp_name &lt;- paste(&quot;industrya&quot;, ind, &quot;locationa&quot;, loc, sep = &quot;_&quot;) # create a subset of the data temp = transactions_agg[transactions_agg$industry == ind &amp; transactions_agg$location == loc, ] combo_row &lt;- cbind(temp_name, nrow(temp)) combo_obs &lt;- rbind(combo_obs, combo_row) assign(temp_name, temp) row &lt;- cbind(temp_name) plotlist_loc_ind_agg &lt;- rbind(plotlist_loc_ind_agg, row) } } plotlist_loc_ind_agg &lt;- as.list(as.character(plotlist_loc_ind_agg$temp_name)) plotlist_loc_ind_agg &lt;- lapply(plotlist_loc_ind_agg, function(plotlist_loc_ind){ ggplot(eval(as.symbol(paste(plotlist_loc_ind))), aes(x = monthly_amount)) + geom_density() + theme_void() + labs (title = paste(plotlist_loc_ind)) }) multiplot(plotlist = plotlist_loc_ind_agg, cols = 5) #this improves the distributions for some of the groupings 2.7 Conclusions A great deal is learnt about the data however the dataset appears to have been previously cleaned and treated for the intended use i.e. mean_amount modelling. There is skewness in all underlying datasets, however taking a mean of the transactions per month renders the distribution of the target dataset’s distribution more toward the normal distribution useful for regression. Little additional cleaning will be undertaken at this stage of the process. "],
["data-cleaning.html", "3 Data Cleaning", " 3 Data Cleaning A summar of all data cleaning based on previous data exploration and understanding, outputting to file to be used in next steps # data cleaning based on previous data exploration and understanding, outputting to file to be used in next steps # import dataset #list of common NA substitution terms # 0 is not included intentionally pot_nas &lt;- c(&quot;&quot;, &quot; &quot;, &quot; &quot;, &quot;.&quot;, &quot;,&quot;, &quot;NaN&quot;, &quot;NAN&quot;, &quot;nan&quot;, &quot;NA&quot;, &quot;na&quot;, &quot;N/A&quot;, &quot;n/a&quot;) # To make checking for missing values, including the read_csv(,na) argument allows us to assign all missing values defined in pot_nas as NA transactions &lt;- read_csv(&quot;./core_data/transactions.csv&quot;, na = pot_nas) ## Parsed with column specification: ## cols( ## date = col_character(), ## customer_id = col_character(), ## industry = col_double(), ## location = col_double(), ## monthly_amount = col_double() ## ) # check names match dictionary names(transactions) ## [1] &quot;date&quot; &quot;customer_id&quot; &quot;industry&quot; &quot;location&quot; ## [5] &quot;monthly_amount&quot; # rename date to trdate due to potential issues with R date function tname &lt;- names(transactions) tname[1] &lt;- &quot;trdate&quot; names(transactions) &lt;- tname #check for NA values if (nrow(transactions) == nrow(na.omit(transactions))){ print(&quot;no missing values&quot;) } else { print(&quot;missing values present&quot;) } ## [1] &quot;no missing values&quot; #data does not appear to have any missing values # date variable not returned in date format transactions$trdate &lt;- as.Date(transactions$trdate, format = &quot;%d/%m/%y&quot;) # update industry, customer and location as they are all factors transactions$customer_id &lt;- as.factor(transactions$customer_id) transactions$industry &lt;- as.factor(transactions$industry) transactions$location &lt;- as.factor(transactions$location) # remove single 0 amount transactions &lt;- transactions %&gt;% filter(monthly_amount != 0) # double check formatting str(transactions) ## Classes &#39;spec_tbl_df&#39;, &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 94247 obs. of 5 variables: ## $ trdate : Date, format: &quot;2013-01-01&quot; &quot;2013-02-01&quot; ... ## $ customer_id : Factor w/ 4464 levels &quot;000a91f3e374e6147d58ed1814247508&quot;,..: 1970 1970 1970 1970 1970 1970 1970 1970 1970 1970 ... ## $ industry : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 8 8 8 8 8 8 8 8 8 8 ... ## $ location : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 9 9 9 9 9 9 9 9 9 9 ... ## $ monthly_amount: num 753851 651548 1138769 659739 770675 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. date = col_character(), ## .. customer_id = col_character(), ## .. industry = col_double(), ## .. location = col_double(), ## .. monthly_amount = col_double() ## .. ) summary(transactions) ## trdate customer_id industry ## Min. :2013-01-01 0023a1e3447fdb31836536cc903f1310: 47 1 :44901 ## 1st Qu.:2014-05-01 006c64491cb8acf2092ce0e0341797fe: 47 2 :22043 ## Median :2015-06-01 0079e3e6d496ad07cee7fd63d3d7c9b2: 47 3 : 8336 ## Mean :2015-03-26 00989c20ff1386dc386d8124ebcba1a5: 47 4 : 5896 ## 3rd Qu.:2016-03-01 00bbd6beace7365b27a913db75fddce9: 47 7 : 3912 ## Max. :2016-11-01 01882513d5fa7c329e940dda99b12147: 47 5 : 3341 ## (Other) :93965 (Other): 5818 ## location monthly_amount ## 2 :21410 Min. : 45986 ## 1 :17659 1st Qu.: 95326 ## 5 : 9703 Median : 179403 ## 4 : 9351 Mean : 395401 ## 6 : 7446 3rd Qu.: 375445 ## 7 : 7317 Max. :100000000 ## (Other):21361 write_csv(transactions, &quot;./core_data/transactions_clean_csv.csv&quot;) save(transactions, file = &quot;./core_data/transactions_clean.Rdata&quot;) "],
["external-data.html", "4 External Data 4.1 Consumer Price index 4.2 ASX Data 4.3 Comined external data 4.4 Unused aditional ideas", " 4 External Data # as the data is of finanical transactions for a recent period, they may relate to other datasets that track finanical metrics, as financial metrics generally have a longer history, models can be more accuratley trained to predict them individually, increasing the accuracy of the final prediction # the company is Australian so australian finanical metrics could be valid # importing data from the relavant periods for potential financial metrics # cpi data is recoreded quaterly by the reserve bank of australia # https://www.rba.gov.au/inflation/measures-cpi.html #consumer price index for australia auscpi &lt;- read_csv(&quot;./integrate_data/CPIdata.csv&quot;) ## Parsed with column specification: ## cols( ## Cdate = col_date(format = &quot;&quot;), ## index = col_double() ## ) names(auscpi) &lt;- c(&quot;cdate&quot;,&quot;index&quot;) # asx metrics were collected already summarised by date from # https://au.investing.com/indices/all-ordinaries-historical-data # asx index of top 500 companies allords &lt;- read_csv(&quot;./integrate_data/ASX All Ordinaries.csv&quot;) ## Parsed with column specification: ## cols( ## Date = col_date(format = &quot;&quot;), ## Price = col_number(), ## Open = col_number(), ## High = col_number(), ## Low = col_number(), ## Vol. = col_character(), ## `Change %` = col_character() ## ) # asx index of the 200 companies after the top 100 in asx all ordinaries smallords &lt;- read_csv(&quot;./integrate_data/ASX Small Ordinaries.csv&quot;) ## Parsed with column specification: ## cols( ## Date = col_date(format = &quot;&quot;), ## Price = col_number(), ## Open = col_number(), ## High = col_number(), ## Low = col_number(), ## Vol. = col_character(), ## `Change %` = col_character() ## ) # asx index of top 20 companies asx20 &lt;- read_csv(&quot;./integrate_data/S&amp;P_ASX 20.csv&quot;) ## Parsed with column specification: ## cols( ## Date = col_date(format = &quot;&quot;), ## Price = col_number(), ## Open = col_number(), ## High = col_number(), ## Low = col_number(), ## Vol. = col_character(), ## `Change %` = col_character() ## ) # asx index of top 200 companies asx200 &lt;- read_csv(&quot;./integrate_data/S&amp;P_ASX 200.csv&quot;) ## Parsed with column specification: ## cols( ## Date = col_date(format = &quot;&quot;), ## Price = col_number(), ## Open = col_number(), ## High = col_number(), ## Low = col_number(), ## Vol. = col_character(), ## `Change %` = col_character() ## ) #import transactions data to subset out the required date range load(file =&quot;./core_data/transactions_clean.Rdata&quot;) trdates_out &lt;- data.frame(unique(transactions$trdate)) names(trdates_out) &lt;- &quot;trdate&quot; 4.1 Consumer Price index #To match the CPIdata to the dataset the additional datapoints are interpolated, the created model follows the extacted data very closely to avoid creating confusion out of the actual data summary(auscpi) ## cdate index ## Min. :2012-12-31 Min. :102.0 ## 1st Qu.:2013-12-31 1st Qu.:104.8 ## Median :2014-12-31 Median :106.6 ## Mean :2014-12-30 Mean :106.3 ## 3rd Qu.:2015-12-31 3rd Qu.:108.2 ## Max. :2016-12-31 Max. :110.0 # check formatting auscpi$cdate &lt;- as.Date(auscpi$cdate) # convert date to numeric value for loess modelling auscpi$ndate &lt;- as.numeric(auscpi$cdate) #transform dates from dataset similarly trdates_out$trdate &lt;- as.Date(trdates_out$trdate) trdates_out$ndate &lt;- as.numeric(trdates_out$trdate) #create loess model for data, loess can create a smoothed curve through or near each point cpi_lm &lt;- loess(index ~ ndate, data = auscpi, span = 0.25, degree = 2) # create a prediction dataframe of predicted points for the same cpi data cpi_predict &lt;- data.frame(index_predict = predict(cpi_lm, auscpi), cdate = auscpi$cdate) #create a dataframe of predicted interpolated points to match the transaction dataset cpi_tdates &lt;- data.frame(trindex_predict = predict(cpi_lm, trdates_out), trdate = trdates_out$trdate) #plot together to check if prediction outcome is as intended cpi_predict_plot &lt;- ggplot(auscpi , aes( x = cdate, y = index)) + geom_point() + geom_line(color =&quot;red&quot;, data = cpi_predict, aes(y = index_predict)) + geom_point(alpha = 0.2, color = &quot;blue&quot;, data = cpi_tdates, aes(x = trdate, y = trindex_predict)) cpi_predict_plot 4.2 ASX Data all_asx &lt;- c(&quot;allords&quot;, &quot;smallords&quot;, &quot;asx20&quot;, &quot;asx200&quot;) combined_asx &lt;- trdates_out names(combined_asx) &lt;- c(&quot;date&quot;, &quot;ndate&quot;) for (index in all_asx) { output &lt;- eval(parse(text = index)) %&gt;% select(&quot;Date&quot;, &quot;Price&quot;, &quot;Vol.&quot;, &quot;Change %&quot;) rename &lt;- c(&quot;date&quot;, paste(index, &quot;price&quot;, sep = &quot;_&quot;), paste(index, &quot;volume&quot;, sep = &quot;_&quot;), paste(index, &quot;change_perc&quot;, sep = &quot;_&quot;)) names(output) &lt;- rename combined_asx &lt;- merge(combined_asx, output, by = &quot;date&quot;, all.x = TRUE) assign(index, output) } str(combined_asx) ## &#39;data.frame&#39;: 47 obs. of 14 variables: ## $ date : Date, format: &quot;2013-01-01&quot; &quot;2013-02-01&quot; ... ## $ ndate : num 15706 15737 15765 15796 15826 ... ## $ allords_price : num 4901 5120 4980 5169 4914 ... ## $ allords_volume : chr &quot;15.86B&quot; &quot;17.90B&quot; &quot;18.55B&quot; &quot;18.02B&quot; ... ## $ allords_change_perc : chr &quot;5.07%&quot; &quot;4.48%&quot; &quot;-2.74%&quot; &quot;3.79%&quot; ... ## $ smallords_price : num 2376 2390 2295 2186 2101 ... ## $ smallords_volume : chr &quot;6.44B&quot; &quot;6.95B&quot; &quot;7.88B&quot; &quot;8.50B&quot; ... ## $ smallords_change_perc: chr &quot;4.17%&quot; &quot;0.58%&quot; &quot;-3.96%&quot; &quot;-4.74%&quot; ... ## $ asx20_price : num 3009 3162 3073 3264 3059 ... ## $ asx20_volume : chr &quot;2.15B&quot; &quot;2.89B&quot; &quot;2.64B&quot; &quot;2.40B&quot; ... ## $ asx20_change_perc : chr &quot;4.96%&quot; &quot;5.10%&quot; &quot;-2.83%&quot; &quot;6.23%&quot; ... ## $ asx200_price : num 4879 5104 4966 5191 4927 ... ## $ asx200_volume : chr &quot;13.14B&quot; &quot;15.11B&quot; &quot;15.31B&quot; &quot;14.58B&quot; ... ## $ asx200_change_perc : chr &quot;4.94%&quot; &quot;4.62%&quot; &quot;-2.70%&quot; &quot;4.52%&quot; ... combined_asx2 &lt;- combined_asx combined_asx2[] &lt;- lapply(combined_asx2, function(x) gsub(&quot;%&quot;, &quot;e-2&quot;, x)) combined_asx2[] &lt;- lapply(combined_asx2, function(x) gsub(&quot;B&quot;, &quot;e9&quot;, x)) combined_asx2[] &lt;- lapply(combined_asx2, function(x) as.numeric(x)) ## Warning in FUN(X[[i]], ...): NAs introduced by coercion combined_asx2 &lt;- combined_asx2[,-1] cor_asx2 &lt;- cor(combined_asx2) corrplot(cor_asx2, method = &quot;number&quot;) combined_asx2 &lt;- combined_asx2 %&gt;% select(-asx200_change_perc, -asx200_volume, -asx200_price, -asx20_change_perc, -smallords_change_perc, -smallords_volume) cor_asx2 &lt;- cor(combined_asx2) corrplot(cor_asx2, method = &quot;number&quot;) combined_asx &lt;- merge(trdates_out, combined_asx2, by = &quot;ndate&quot;) 4.3 Comined external data #correlation combined_external &lt;- merge(combined_asx, cpi_tdates, by = &quot;trdate&quot;) cor_ext &lt;- cor(combined_external[-1]) corrplot(cor_ext, method = &quot;number&quot;) #high corrolation between the cpi predictions and the date as a numerica, they likely both have y=x increases predominantly, for now the variable will remain save(combined_external, file = &quot;./integrate_data/fin_mets.Rdata&quot;) 4.4 Unused aditional ideas # School Holidays a_i1_l1$shols &lt;- &quot;0&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Apr&quot; , &quot;shols&quot;] &lt;- 2.5 a_i1_l1[a_i1_l1$trmonth == &quot;Jul&quot; , &quot;shols&quot;] &lt;- 2 a_i1_l1[a_i1_l1$trmonth == &quot;Sep&quot; , &quot;shols&quot;] &lt;- 1 a_i1_l1[a_i1_l1$trmonth == &quot;Oct&quot; , &quot;shols&quot;] &lt;- 1 a_i1_l1[a_i1_l1$trmonth == &quot;Dec&quot; , &quot;shols&quot;] &lt;- 2.5 a_i1_l1[a_i1_l1$trmonth == &quot;Jan&quot; , &quot;shols&quot;] &lt;- 4 a_i1_l1$shols &lt;- as.numeric(a_i1_l1$shols) summary(a_i1_l1$shols) # inflation # https://www.statista.com/statistics/271845/inflation-rate-in-australia/ inflation &lt;- tibble(year = c(2013, 2014, 2015, 2016), inflperc = c(2.45, 2.51, 1.51, 1.25)) # gdp growth # https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG?end=2016&amp;locations=AU&amp;start=2013 gdpgrowth &lt;- tibble(year = c(2013, 2014, 2015, 2016), growthperc = c(2.585,2.533,2.193,2.771)) gdpgrowth &lt;- gdpgrowth %&gt;% slice(rep(1:n(), each = 12)) gdpgrowth$growthperc &lt;- gdpgrowth$growthperc/12/100 gdpgrowth &lt;- gdpgrowth[-48,] gdpgrowth$growthperccumul &lt;- gdpgrowth$growthperc gdpgrowth &lt;- data.frame(gdpgrowth) for (inter in 2:nrow(gdpgrowth)) { gdpgrowth[inter,3] &lt;- gdpgrowth[[inter,2]] + gdpgrowth[[inter-1,3]] } # Seasons a_i1_l1$seasons &lt;- &quot;spring&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Dec&quot; | a_i1_l1$trmonth == &quot;Jan&quot; | a_i1_l1$trmonth == &quot;Feb&quot; , &quot;seasons&quot;] &lt;- &quot;summer&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Mar&quot; | a_i1_l1$trmonth == &quot;Apr&quot; | a_i1_l1$trmonth == &quot;May&quot; , &quot;seasons&quot;] &lt;- &quot;autumn&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Jun&quot; | a_i1_l1$trmonth == &quot;Jul&quot; | a_i1_l1$trmonth == &quot;Aug&quot; , &quot;seasons&quot;] &lt;- &quot;winter&quot; a_i1_l1$seasons &lt;- as.ordered(a_i1_l1$seasons) # aus fiscal quarters a_i1_l1$fisquarters &lt;- &quot;fQ4&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Jul&quot; | a_i1_l1$trmonth == &quot;Aug&quot; | a_i1_l1$trmonth == &quot;Sep&quot; , &quot;fisquarters&quot;] &lt;- &quot;fQ1&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Oct&quot; | a_i1_l1$trmonth == &quot;Nov&quot; | a_i1_l1$trmonth == &quot;Dec&quot; , &quot;fisquarters&quot;] &lt;- &quot;fQ2&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Jan&quot; | a_i1_l1$trmonth == &quot;Feb&quot; | a_i1_l1$trmonth == &quot;Mar&quot; , &quot;fisquarters&quot;] &lt;- &quot;fQ3&quot; a_i1_l1$fisquarters &lt;- as.ordered(a_i1_l1$fisquarters) # calendar quarters a_i1_l1$calquarters &lt;- &quot;cQ4&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Jan&quot; | a_i1_l1$trmonth == &quot;Feb&quot; | a_i1_l1$trmonth == &quot;Mar&quot; , &quot;calquarters&quot;] &lt;- &quot;cQ1&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Apr&quot; | a_i1_l1$trmonth == &quot;May&quot; | a_i1_l1$trmonth == &quot;Jun&quot; , &quot;calquarters&quot;] &lt;- &quot;cQ2&quot; a_i1_l1[a_i1_l1$trmonth == &quot;Jul&quot; | a_i1_l1$trmonth == &quot;Aug&quot; | a_i1_l1$trmonth == &quot;Sep&quot; , &quot;calquarters&quot;] &lt;- &quot;cQ3&quot; a_i1_l1$calquarters &lt;- as.ordered(a_i1_l1$calquarters) "],
["data-preparation.html", "5 Data Preparation 5.1 Constructed Variables 5.2 Integrated variables 5.3 Output 5.4 unused", " 5 Data Preparation #import cleaned data load(file =&quot;./core_data/transactions_clean.Rdata&quot;) #import external data load(file = &quot;./integrate_data/fin_mets.Rdata&quot;) # create aggregate dataset as per brief # intial aggregate function switched to dplyr so a number of transactions summarised can be appeneded # transactions_agg2 &lt;- aggregate(.~trdate + location + industry, transactions[-2], FUN = mean) transactions_agg &lt;- transactions %&gt;% group_by(industry, location, trdate) %&gt;% summarise(mean_amount = mean(monthly_amount) , ntrans = n(), sdtrans = sd(monthly_amount)) %&gt;% select(trdate, everything()) 5.1 Constructed Variables # months transactions_agg$trmonth &lt;- lubridate::month(transactions_agg$trdate, label = TRUE, abbr = TRUE) # change months into dummy variables (0/1) tmonths &lt;- unique(transactions_agg$trmonth) # df_months &lt;- data.frame(matrix(data = 0, ncol = 12, nrow = 3886)) # names(df_months) &lt;- months # transactions_agg &lt;- cbind(transactions_agg, df_months) # for (mnth in months) { # transactions_agg[transactions_agg$trmonth == mnth, (eval(as.symbol(paste(&quot;transactions_agg&quot;, mnth, sep=&quot;$&quot;))] &lt;- 1 # } for (mnth in tmonths) { transactions_agg[transactions_agg$trmonth == mnth, mnth] &lt;- 1 transactions_agg[transactions_agg$trmonth != mnth, mnth] &lt;- 0 } # customers # was applied in the aggregate step 5.2 Integrated variables # join transactions dataset with external data by date transactions_prepd &lt;- merge(transactions_agg, combined_external, by = &quot;trdate&quot;) %&gt;% arrange(industry, location, trdate) 5.3 Output str(transactions_prepd) ## &#39;data.frame&#39;: 3886 obs. of 27 variables: ## $ trdate : Date, format: &quot;2013-01-01&quot; &quot;2013-02-01&quot; ... ## $ industry : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ location : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ mean_amount : num 136081 152964 158481 152341 170330 ... ## $ ntrans : int 66 70 72 71 72 73 79 74 75 77 ... ## $ sdtrans : num 118121 143495 159993 138492 172757 ... ## $ trmonth : Ord.factor w/ 12 levels &quot;Jan&quot;&lt;&quot;Feb&quot;&lt;&quot;Mar&quot;&lt;..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Jan : num 1 0 0 0 0 0 0 0 0 0 ... ## $ Feb : num 0 1 0 0 0 0 0 0 0 0 ... ## $ Mar : num 0 0 1 0 0 0 0 0 0 0 ... ## $ Apr : num 0 0 0 1 0 0 0 0 0 0 ... ## $ May : num 0 0 0 0 1 0 0 0 0 0 ... ## $ Jun : num 0 0 0 0 0 1 0 0 0 0 ... ## $ Jul : num 0 0 0 0 0 0 1 0 0 0 ... ## $ Aug : num 0 0 0 0 0 0 0 1 0 0 ... ## $ Sep : num 0 0 0 0 0 0 0 0 1 0 ... ## $ Oct : num 0 0 0 0 0 0 0 0 0 1 ... ## $ Nov : num 0 0 0 0 0 0 0 0 0 0 ... ## $ Dec : num 0 0 0 0 0 0 0 0 0 0 ... ## $ ndate : num 15706 15737 15765 15796 15826 ... ## $ allords_price : num 4901 5120 4980 5169 4914 ... ## $ allords_volume : num 1.59e+10 1.79e+10 1.86e+10 1.80e+10 2.23e+10 ... ## $ allords_change_perc: num 0.0507 0.0448 -0.0274 0.0379 -0.0493 -0.0282 0.0545 0.0178 0.018 0.0388 ... ## $ smallords_price : num 2376 2390 2295 2186 2101 ... ## $ asx20_price : num 3009 3162 3073 3264 3059 ... ## $ asx20_volume : num 2.15e+09 2.89e+09 2.64e+09 2.40e+09 2.84e+09 2.87e+09 2.50e+09 2.57e+09 2.34e+09 2.18e+09 ... ## $ trindex_predict : num 102 102 102 102 103 ... summary(transactions_prepd) ## trdate industry location mean_amount ## Min. :2013-01-01 1 : 470 1 : 437 Min. : 63982 ## 1st Qu.:2013-12-08 2 : 470 2 : 423 1st Qu.: 233024 ## Median :2014-12-01 4 : 470 7 : 423 Median : 362811 ## Mean :2014-12-07 7 : 470 4 : 405 Mean : 1015507 ## 3rd Qu.:2015-12-01 8 : 470 5 : 388 3rd Qu.: 554968 ## Max. :2016-11-01 5 : 465 9 : 388 Max. :49971497 ## (Other):1071 (Other):1422 ## ntrans sdtrans trmonth Jan ## Min. : 1.00 Min. : 1124 Nov : 334 Min. :0.00000 ## 1st Qu.: 4.00 1st Qu.: 154820 Oct : 333 1st Qu.:0.00000 ## Median : 9.00 Median : 247489 Sep : 332 Median :0.00000 ## Mean : 24.25 Mean : 1088709 May : 331 Mean :0.08415 ## 3rd Qu.: 20.00 3rd Qu.: 445943 Feb : 330 3rd Qu.:0.00000 ## Max. :281.00 Max. :49792749 Mar : 330 Max. :1.00000 ## NA&#39;s :322 (Other):1896 ## Feb Mar Apr May ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.00000 1st Qu.:0.00000 1st Qu.:0.00000 1st Qu.:0.00000 ## Median :0.00000 Median :0.00000 Median :0.00000 Median :0.00000 ## Mean :0.08492 Mean :0.08492 Mean :0.08492 Mean :0.08518 ## 3rd Qu.:0.00000 3rd Qu.:0.00000 3rd Qu.:0.00000 3rd Qu.:0.00000 ## Max. :1.00000 Max. :1.00000 Max. :1.00000 Max. :1.00000 ## ## Jun Jul Aug Sep ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :0.00000 ## 1st Qu.:0.00000 1st Qu.:0.00000 1st Qu.:0.00000 1st Qu.:0.00000 ## Median :0.00000 Median :0.00000 Median :0.00000 Median :0.00000 ## Mean :0.08492 Mean :0.08492 Mean :0.08492 Mean :0.08543 ## 3rd Qu.:0.00000 3rd Qu.:0.00000 3rd Qu.:0.00000 3rd Qu.:0.00000 ## Max. :1.00000 Max. :1.00000 Max. :1.00000 Max. :1.00000 ## ## Oct Nov Dec ndate ## Min. :0.00000 Min. :0.00000 Min. :0.00000 Min. :15706 ## 1st Qu.:0.00000 1st Qu.:0.00000 1st Qu.:0.00000 1st Qu.:16048 ## Median :0.00000 Median :0.00000 Median :0.00000 Median :16405 ## Mean :0.08569 Mean :0.08595 Mean :0.06408 Mean :16411 ## 3rd Qu.:0.00000 3rd Qu.:0.00000 3rd Qu.:0.00000 3rd Qu.:16770 ## Max. :1.00000 Max. :1.00000 Max. :1.00000 Max. :17106 ## ## allords_price allords_volume allords_change_perc smallords_price ## Min. :4775 Min. :1.453e+10 Min. :-0.080900 Min. :1944 ## 1st Qu.:5169 1st Qu.:1.637e+10 1st Qu.:-0.022200 1st Qu.:2101 ## Median :5353 Median :1.785e+10 Median : 0.000600 Median :2175 ## Mean :5350 Mean :1.773e+10 Mean : 0.004109 Mean :2190 ## 3rd Qu.:5505 3rd Qu.:1.912e+10 3rd Qu.: 0.038800 3rd Qu.:2265 ## Max. :5898 Max. :2.230e+10 Max. : 0.062800 Max. :2482 ## ## asx20_price asx20_volume trindex_predict ## Min. :2803 Min. :1.750e+09 Min. :102.0 ## 1st Qu.:3073 1st Qu.:2.280e+09 1st Qu.:104.6 ## Median :3261 Median :2.550e+09 Median :106.5 ## Mean :3263 Mean :2.537e+09 Mean :106.2 ## 3rd Qu.:3423 3rd Qu.:2.820e+09 3rd Qu.:108.2 ## Max. :3714 Max. :3.400e+09 Max. :109.6 ## save(transactions_prepd, file = &quot;./core_data/transactions_prepd.Rdata&quot;) 5.4 unused # year transactions_agg$tryear &lt;- lubridate::year(transactions_agg$trdate) years &lt;- unique(transactions_agg$tryear) for (yr in years) { transactions_agg[transactions_agg$tryear == yr, paste(yr)] &lt;- 1 transactions_agg[transactions_agg$tryear != yr, paste(yr)] &lt;- 0 } # yearly arc # wanteed to plot a 1/2 sine curve based on trmonth but subsitututed this as a test yarc &lt;- data.frame(trmonth = tmonths, yarc = c(1,2,3,4,5,6,6,5,4,3,2,1)) transactions_agg &lt;- merge(transactions_agg, yarc, by = &quot;trmonth&quot;) "],
["modelling.html", "6 Modelling 6.1 Test Setup 6.2 Initial Model 6.3 All Ordinaries Volume 6.4 Experiment", " 6 Modelling # import prepped data load(file = &quot;./core_data/transactions_prepd.Rdata&quot;) transactions_prepd &lt;- transactions_prepd %&gt;% select(-sdtrans) 6.1 Test Setup pi1l1 &lt;- transactions_prepd %&gt;% filter(industry == 1, location == 1) %&gt;% select(-industry, -location, -trdate) %&gt;% #removing trdate and switching to ndate for ease of calculation arrange(ndate) %&gt;% select(ndate, everything()) trainrows &lt;- ceiling(0.75*nrow(pi1l1)) train_i1l1 &lt;- pi1l1[1:trainrows,] test_i1l1 &lt;- pi1l1[(trainrows+1):nrow(pi1l1),] baseplot_train_i1l1 &lt;- ggplot(train_i1l1, aes(x = ndate, y = mean_amount)) + geom_point() + geom_line() + geom_text(aes(label = trmonth, vjust=-1)) baseplot_train_i1l1 6.2 Initial Model #remove trmonth as we have dummy variables in place train_i1l1 &lt;- train_i1l1 %&gt;% select(-trmonth) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4848.4 -1493.8 198.9 1899.1 5118.7 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.343e+05 2.834e+05 -2.591 0.020459 * ## ndate 1.110e+02 5.288e+01 2.099 0.053111 . ## ntrans -3.719e+02 3.852e+02 -0.965 0.349673 ## Jan 2.215e+01 4.046e+03 0.005 0.995704 ## Feb 2.145e+04 4.502e+03 4.765 0.000251 *** ## Mar 1.976e+04 4.865e+03 4.061 0.001025 ** ## Apr 9.349e+03 3.839e+03 2.435 0.027845 * ## May 1.731e+04 4.228e+03 4.094 0.000958 *** ## Jun 4.862e+03 4.541e+03 1.071 0.301279 ## Jul 1.779e+04 3.714e+03 4.791 0.000238 *** ## Aug 1.354e+04 4.097e+03 3.305 0.004809 ** ## Sep 5.375e+03 4.205e+03 1.278 0.220561 ## Oct 2.398e+04 4.020e+03 5.966 2.59e-05 *** ## Nov 1.428e+04 3.899e+03 3.662 0.002310 ** ## Dec NA NA NA NA ## allords_price -6.994e+01 4.079e+01 -1.715 0.106948 ## allords_volume 1.443e-06 1.021e-06 1.413 0.177994 ## allords_change_perc -5.944e+04 3.880e+04 -1.532 0.146411 ## smallords_price 1.903e+01 2.563e+01 0.742 0.469341 ## asx20_price 7.543e+01 4.564e+01 1.652 0.119217 ## asx20_volume -2.657e-06 6.040e-06 -0.440 0.666245 ## trindex_predict -7.799e+03 5.913e+03 -1.319 0.206972 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3959 on 15 degrees of freedom ## Multiple R-squared: 0.9452, Adjusted R-squared: 0.8722 ## F-statistic: 12.94 on 20 and 15 DF, p-value: 3.733e-06 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) ## Warning in predict.lm(lm_train_i1l1, train_i1l1): prediction from a rank- ## deficient fit may be misleading baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #AR^2 of 0.8722 but with some issues cor_train_i1l1 &lt;- cor(train_i1l1) corrplot(cor_train_i1l1, method = &quot;number&quot;) # highlights an unnoticed corrolation between transactions and date load(file = &quot;./core_data/transactions_clean.Rdata&quot;) t11 &lt;- transactions %&gt;% filter(industry == 1, location == 1) as.data.frame(table(t11$trdate)) ## Var1 Freq ## 1 2013-01-01 66 ## 2 2013-02-01 70 ## 3 2013-03-01 72 ## 4 2013-04-01 71 ## 5 2013-05-01 72 ## 6 2013-06-01 73 ## 7 2013-07-01 79 ## 8 2013-08-01 74 ## 9 2013-09-01 75 ## 10 2013-10-01 77 ## 11 2013-11-01 77 ## 12 2013-12-01 76 ## 13 2014-01-01 77 ## 14 2014-02-01 80 ## 15 2014-03-01 76 ## 16 2014-04-01 79 ## 17 2014-05-01 78 ## 18 2014-06-01 77 ## 19 2014-07-01 86 ## 20 2014-08-01 95 ## 21 2014-09-01 90 ## 22 2014-10-01 98 ## 23 2014-11-01 100 ## 24 2014-12-01 99 ## 25 2015-01-01 104 ## 26 2015-02-01 110 ## 27 2015-03-01 111 ## 28 2015-04-01 110 ## 29 2015-05-01 116 ## 30 2015-06-01 116 ## 31 2015-07-01 116 ## 32 2015-08-01 118 ## 33 2015-09-01 120 ## 34 2015-10-01 116 ## 35 2015-11-01 115 ## 36 2015-12-01 112 ## 37 2016-01-01 112 ## 38 2016-02-01 113 ## 39 2016-03-01 112 ## 40 2016-04-01 112 ## 41 2016-05-01 115 ## 42 2016-06-01 121 ## 43 2016-07-01 126 ## 44 2016-08-01 124 ## 45 2016-09-01 127 ## 46 2016-10-01 126 ## 47 2016-11-01 132 # this verifies the correlation, this shows as the company has grown and the number of customers has increased, # so has the mean spend per customer, or newer customers are spending more # reducing variables with high correllation &amp; irrelevant months # ntrans - ndate, train_i1l1 &lt;- train_i1l1 %&gt;% select(-trindex_predict, -asx20_price, -Jun, -Sep) # compare between ndate and ntrans lm_train_i1l1 &lt;- lm(mean_amount ~. -ndate, data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ . - ndate, data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7443.4 -2551.8 148.2 3296.3 6315.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.736e+05 5.070e+04 3.424 0.002846 ** ## ntrans 4.541e+02 1.081e+02 4.202 0.000483 *** ## Jan -1.119e+04 4.990e+03 -2.242 0.037071 * ## Feb 1.153e+04 4.771e+03 2.418 0.025842 * ## Mar 1.130e+04 4.385e+03 2.576 0.018503 * ## Apr -3.450e+02 4.386e+03 -0.079 0.938135 ## May 9.114e+03 3.741e+03 2.436 0.024850 * ## Jul 8.579e+03 4.677e+03 1.834 0.082341 . ## Aug 6.348e+03 3.673e+03 1.728 0.100171 ## Oct 1.486e+04 4.479e+03 3.318 0.003612 ** ## Nov 5.054e+03 4.195e+03 1.205 0.243090 ## Dec -4.862e+03 4.220e+03 -1.152 0.263500 ## allords_price -5.691e+00 5.559e+00 -1.024 0.318745 ## allords_volume 1.627e-06 9.402e-07 1.730 0.099762 . ## allords_change_perc -4.714e+04 4.416e+04 -1.068 0.299118 ## smallords_price -1.482e+01 1.467e+01 -1.010 0.325000 ## asx20_volume -9.352e-06 4.795e-06 -1.951 0.066024 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4880 on 19 degrees of freedom ## Multiple R-squared: 0.8946, Adjusted R-squared: 0.8059 ## F-statistic: 10.08 on 16 and 19 DF, p-value: 3.875e-06 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #AR^2 0.8059 lm_train_i1l1_high &lt;- lm(mean_amount ~. -ntrans, data = train_i1l1) summary(lm_train_i1l1_high) ## ## Call: ## lm(formula = mean_amount ~ . - ntrans, data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6398.0 -1591.3 -423.4 2054.8 5275.9 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.275e+05 8.016e+04 -2.838 0.010511 * ## ndate 2.637e+01 4.360e+00 6.049 8.1e-06 *** ## Jan -7.212e+03 3.942e+03 -1.830 0.083045 . ## Feb 1.294e+04 3.885e+03 3.330 0.003516 ** ## Mar 1.289e+04 3.529e+03 3.654 0.001689 ** ## Apr 3.341e+03 3.547e+03 0.942 0.358041 ## May 1.043e+04 3.031e+03 3.442 0.002734 ** ## Jul 1.136e+04 3.768e+03 3.016 0.007099 ** ## Aug 7.556e+03 2.953e+03 2.559 0.019208 * ## Oct 1.587e+04 3.603e+03 4.406 0.000304 *** ## Nov 7.335e+03 3.254e+03 2.254 0.036167 * ## Dec -5.159e+03 3.421e+03 -1.508 0.147994 ## allords_price -7.721e+00 4.460e+00 -1.731 0.099613 . ## allords_volume 1.689e-06 7.538e-07 2.241 0.037174 * ## allords_change_perc -4.289e+04 3.584e+04 -1.197 0.246118 ## smallords_price -1.039e+01 1.181e+01 -0.880 0.389959 ## asx20_volume -4.224e-06 3.020e-06 -1.399 0.178006 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3962 on 19 degrees of freedom ## Multiple R-squared: 0.9305, Adjusted R-squared: 0.872 ## F-statistic: 15.9 on 16 and 19 DF, p-value: 9.484e-08 plot(lm_train_i1l1_high) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1_high, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #AR^2 0.872 # removing ntrans train_i1l1 &lt;- train_i1l1 %&gt;% select(-ntrans) # checking corroloation once again cor_train_i1l1 &lt;- cor(train_i1l1) corrplot(cor_train_i1l1, method = &quot;number&quot;) # test between as20_volume and allords_volume lm_train_i1l1 &lt;- lm(mean_amount ~. -asx20_volume, data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ . - asx20_volume, data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8016 -1486 -206 1578 5984 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.755e+05 7.271e+04 -2.414 0.025470 * ## ndate 2.290e+01 3.671e+00 6.239 4.3e-06 *** ## Jan -6.266e+03 3.975e+03 -1.576 0.130648 ## Feb 1.205e+04 3.923e+03 3.071 0.006033 ** ## Mar 1.355e+04 3.579e+03 3.786 0.001159 ** ## Apr 3.996e+03 3.599e+03 1.110 0.279999 ## May 1.074e+04 3.094e+03 3.472 0.002404 ** ## Jul 1.149e+04 3.856e+03 2.979 0.007419 ** ## Aug 7.643e+03 3.022e+03 2.529 0.019963 * ## Oct 1.600e+04 3.687e+03 4.340 0.000318 *** ## Nov 8.716e+03 3.174e+03 2.747 0.012440 * ## Dec -4.567e+03 3.475e+03 -1.314 0.203646 ## allords_price -6.042e+00 4.397e+00 -1.374 0.184572 ## allords_volume 1.280e-06 7.112e-07 1.800 0.086938 . ## allords_change_perc -3.540e+04 3.627e+04 -0.976 0.340828 ## smallords_price -1.427e+01 1.176e+01 -1.213 0.239085 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4056 on 20 degrees of freedom ## Multiple R-squared: 0.9234, Adjusted R-squared: 0.8659 ## F-statistic: 16.06 on 15 and 20 DF, p-value: 5.52e-08 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.8659 lm_train_i1l1 &lt;- lm(mean_amount ~. -allords_volume, data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ . - allords_volume, data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7330.6 -1691.6 -253.5 2189.1 6051.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.114e+05 6.703e+04 -1.662 0.11213 ## ndate 2.324e+01 4.526e+00 5.135 5.04e-05 *** ## Jan -1.129e+04 3.833e+03 -2.945 0.00801 ** ## Feb 1.337e+04 4.253e+03 3.143 0.00513 ** ## Mar 1.662e+04 3.410e+03 4.876 9.15e-05 *** ## Apr 6.890e+02 3.664e+03 0.188 0.85275 ## May 1.181e+04 3.253e+03 3.629 0.00167 ** ## Jul 9.808e+03 4.058e+03 2.417 0.02534 * ## Aug 8.767e+03 3.182e+03 2.755 0.01221 * ## Oct 1.504e+04 3.927e+03 3.829 0.00105 ** ## Nov 5.575e+03 3.460e+03 1.611 0.12285 ## Dec -7.226e+03 3.610e+03 -2.002 0.05906 . ## allords_price -1.034e+01 4.717e+00 -2.191 0.04045 * ## allords_change_perc -1.667e+04 3.712e+04 -0.449 0.65831 ## smallords_price -2.316e+01 1.134e+01 -2.042 0.05457 . ## asx20_volume -1.600e-06 3.051e-06 -0.524 0.60573 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4343 on 20 degrees of freedom ## Multiple R-squared: 0.9121, Adjusted R-squared: 0.8463 ## F-statistic: 13.84 on 15 and 20 DF, p-value: 2.013e-07 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.8463 train_i1l1 &lt;- train_i1l1 %&gt;% select(-asx20_volume) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8016 -1486 -206 1578 5984 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.755e+05 7.271e+04 -2.414 0.025470 * ## ndate 2.290e+01 3.671e+00 6.239 4.3e-06 *** ## Jan -6.266e+03 3.975e+03 -1.576 0.130648 ## Feb 1.205e+04 3.923e+03 3.071 0.006033 ** ## Mar 1.355e+04 3.579e+03 3.786 0.001159 ** ## Apr 3.996e+03 3.599e+03 1.110 0.279999 ## May 1.074e+04 3.094e+03 3.472 0.002404 ** ## Jul 1.149e+04 3.856e+03 2.979 0.007419 ** ## Aug 7.643e+03 3.022e+03 2.529 0.019963 * ## Oct 1.600e+04 3.687e+03 4.340 0.000318 *** ## Nov 8.716e+03 3.174e+03 2.747 0.012440 * ## Dec -4.567e+03 3.475e+03 -1.314 0.203646 ## allords_price -6.042e+00 4.397e+00 -1.374 0.184572 ## allords_volume 1.280e-06 7.112e-07 1.800 0.086938 . ## allords_change_perc -3.540e+04 3.627e+04 -0.976 0.340828 ## smallords_price -1.427e+01 1.176e+01 -1.213 0.239085 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4056 on 20 degrees of freedom ## Multiple R-squared: 0.9234, Adjusted R-squared: 0.8659 ## F-statistic: 16.06 on 15 and 20 DF, p-value: 5.52e-08 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) # the adjusted r-squared is decreasing as variables that correlated with ndate are removed, ndate could be related as a polynomial train_i1l1 &lt;- train_i1l1 %&gt;% select(-allords_change_perc, -smallords_price) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9830.9 -2048.1 412.5 2021.8 6260.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.583e+05 4.583e+04 -5.637 1.14e-05 *** ## ndate 2.682e+01 3.139e+00 8.542 1.95e-08 *** ## Jan -7.968e+03 3.714e+03 -2.145 0.04321 * ## Feb 7.760e+03 3.361e+03 2.309 0.03072 * ## Mar 1.181e+04 3.425e+03 3.447 0.00230 ** ## Apr 2.830e+03 3.514e+03 0.805 0.42934 ## May 9.750e+03 3.142e+03 3.103 0.00519 ** ## Jul 8.627e+03 3.159e+03 2.731 0.01219 * ## Aug 6.505e+03 3.039e+03 2.141 0.04362 * ## Oct 1.271e+04 3.064e+03 4.149 0.00042 *** ## Nov 8.527e+03 3.263e+03 2.613 0.01587 * ## Dec -6.692e+03 3.182e+03 -2.103 0.04712 * ## allords_price -8.681e+00 4.344e+00 -1.998 0.05818 . ## allords_volume 1.506e-06 6.598e-07 2.282 0.03251 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4231 on 22 degrees of freedom ## Multiple R-squared: 0.9083, Adjusted R-squared: 0.854 ## F-statistic: 16.75 on 13 and 22 DF, p-value: 1.782e-08 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) # 0.854 train_i1l1 &lt;- train_i1l1 %&gt;% select(-Apr) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10060.5 -2060.6 377.3 2434.5 5783.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.451e+05 4.247e+04 -5.772 7.03e-06 *** ## ndate 2.614e+01 3.001e+00 8.711 9.66e-09 *** ## Jan -9.456e+03 3.197e+03 -2.957 0.007059 ** ## Feb 6.789e+03 3.113e+03 2.181 0.039667 * ## Mar 1.115e+04 3.301e+03 3.377 0.002597 ** ## May 8.906e+03 2.940e+03 3.029 0.005963 ** ## Jul 7.581e+03 2.857e+03 2.654 0.014190 * ## Aug 5.779e+03 2.879e+03 2.007 0.056636 . ## Oct 1.183e+04 2.839e+03 4.166 0.000372 *** ## Nov 7.447e+03 2.952e+03 2.523 0.019009 * ## Dec -7.752e+03 2.875e+03 -2.696 0.012883 * ## allords_price -8.241e+00 4.276e+00 -1.927 0.066410 . ## allords_volume 1.306e-06 6.064e-07 2.153 0.042062 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4199 on 23 degrees of freedom ## Multiple R-squared: 0.9056, Adjusted R-squared: 0.8563 ## F-statistic: 18.38 on 12 and 23 DF, p-value: 5.428e-09 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.8563 train_i1l1_ok &lt;- train_i1l1 lm_train_i1l1 &lt;- lm(mean_amount ~. + sin(2*pi*ndate), data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ . + sin(2 * pi * ndate), data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10134.2 -2092.0 339.9 2384.7 5845.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.412e+05 4.491e+04 -5.370 2.16e-05 *** ## ndate 2.589e+01 3.150e+00 8.219 3.76e-08 *** ## Jan -9.865e+03 3.483e+03 -2.832 0.009693 ** ## Feb 6.106e+03 3.775e+03 1.618 0.120004 ## Mar 1.079e+04 3.536e+03 3.050 0.005870 ** ## May 8.611e+03 3.125e+03 2.755 0.011551 * ## Jul 7.357e+03 2.989e+03 2.462 0.022148 * ## Aug 5.727e+03 2.941e+03 1.947 0.064355 . ## Oct 1.184e+04 2.896e+03 4.090 0.000485 *** ## Nov 7.264e+03 3.060e+03 2.373 0.026779 * ## Dec -7.672e+03 2.942e+03 -2.608 0.016063 * ## allords_price -8.327e+00 4.369e+00 -1.906 0.069808 . ## allords_volume 1.330e-06 6.228e-07 2.136 0.044099 * ## sin(2 * pi * ndate) -8.928e+13 2.669e+14 -0.334 0.741208 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4282 on 22 degrees of freedom ## Multiple R-squared: 0.906, Adjusted R-squared: 0.8505 ## F-statistic: 16.32 on 13 and 22 DF, p-value: 2.292e-08 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) lm_train_i1l1 &lt;- lm(mean_amount ~. + allords_volume*ndate*Dec, data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ . + allords_volume * ndate * Dec, ## data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9879.9 -1327.5 473.9 2186.7 6231.0 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.160e+05 4.913e+05 -1.050 0.306121 ## ndate 4.321e+01 3.050e+01 1.417 0.171877 ## Jan -9.290e+03 3.416e+03 -2.720 0.013193 * ## Feb 7.236e+03 3.309e+03 2.187 0.040797 * ## Mar 1.165e+04 3.508e+03 3.320 0.003420 ** ## May 8.644e+03 3.181e+03 2.717 0.013263 * ## Jul 7.445e+03 3.027e+03 2.460 0.023145 * ## Aug 5.905e+03 3.028e+03 1.950 0.065357 . ## Oct 1.184e+04 2.982e+03 3.972 0.000751 *** ## Nov 7.258e+03 3.107e+03 2.336 0.030038 * ## Dec 8.704e+04 1.594e+05 0.546 0.591142 ## allords_price -9.023e+00 4.599e+00 -1.962 0.063807 . ## allords_volume 1.597e-05 2.697e-05 0.592 0.560402 ## ndate:allords_volume -9.114e-10 1.668e-09 -0.546 0.590820 ## Dec:allords_volume 2.802e-06 4.041e-06 0.694 0.495918 ## ndate:Dec -8.643e+00 1.126e+01 -0.768 0.451687 ## ndate:Dec:allords_volume NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4408 on 20 degrees of freedom ## Multiple R-squared: 0.9095, Adjusted R-squared: 0.8416 ## F-statistic: 13.4 on 15 and 20 DF, p-value: 2.669e-07 plot(lm_train_i1l1) ## Warning: not plotting observations with leverage one: ## 12, 24, 36 ## Warning: not plotting observations with leverage one: ## 12, 24, 36 lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) ## Warning in predict.lm(lm_train_i1l1, train_i1l1): prediction from a rank- ## deficient fit may be misleading baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) train_i1l1 &lt;- train_i1l1 %&gt;% select(-Aug, -allords_price) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10850.8 -1751.8 -71.1 1739.0 7622.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.531e+05 4.610e+04 -5.490 1.06e-05 *** ## ndate 2.310e+01 2.632e+00 8.775 4.18e-09 *** ## Jan -8.622e+03 3.332e+03 -2.588 0.015865 * ## Feb 3.020e+03 2.984e+03 1.012 0.321186 ## Mar 6.862e+03 3.098e+03 2.215 0.036089 * ## May 6.041e+03 2.974e+03 2.031 0.052989 . ## Jul 5.331e+03 2.968e+03 1.796 0.084527 . ## Oct 9.858e+03 2.968e+03 3.321 0.002757 ** ## Nov 7.776e+03 3.050e+03 2.549 0.017307 * ## Dec -8.189e+03 3.036e+03 -2.697 0.012336 * ## allords_volume 2.148e-06 5.266e-07 4.079 0.000405 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4581 on 25 degrees of freedom ## Multiple R-squared: 0.8778, Adjusted R-squared: 0.8289 ## F-statistic: 17.95 on 10 and 25 DF, p-value: 5.018e-09 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) # 0.8289, train_i1l1 &lt;- train_i1l1 %&gt;% select(-Feb) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10706.7 -1791.3 82.8 2242.9 7118.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.484e+05 4.589e+04 -5.414 1.13e-05 *** ## ndate 2.282e+01 2.619e+00 8.713 3.44e-09 *** ## Jan -9.193e+03 3.285e+03 -2.798 0.009547 ** ## Mar 6.187e+03 3.027e+03 2.044 0.051183 . ## May 5.416e+03 2.911e+03 1.861 0.074119 . ## Jul 4.753e+03 2.914e+03 1.631 0.114872 ## Oct 9.294e+03 2.917e+03 3.186 0.003726 ** ## Nov 7.258e+03 3.008e+03 2.413 0.023186 * ## Dec -8.703e+03 2.995e+03 -2.906 0.007380 ** ## allords_volume 2.175e-06 5.262e-07 4.134 0.000329 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4584 on 26 degrees of freedom ## Multiple R-squared: 0.8728, Adjusted R-squared: 0.8287 ## F-statistic: 19.82 on 9 and 26 DF, p-value: 1.745e-09 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) # 0.8287 train_i1l1 &lt;- train_i1l1 %&gt;% select(-Jul) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11051.5 -1682.9 193.3 2181.3 7929.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.465e+05 4.727e+04 -5.215 1.71e-05 *** ## ndate 2.284e+01 2.698e+00 8.464 4.46e-09 *** ## Jan -1.019e+04 3.325e+03 -3.066 0.00489 ** ## Mar 5.546e+03 3.092e+03 1.794 0.08408 . ## May 4.674e+03 2.962e+03 1.578 0.12617 ## Oct 8.494e+03 2.962e+03 2.867 0.00793 ** ## Nov 6.345e+03 3.045e+03 2.083 0.04681 * ## Dec -9.602e+03 3.033e+03 -3.166 0.00381 ** ## allords_volume 2.093e-06 5.396e-07 3.879 0.00061 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4722 on 27 degrees of freedom ## Multiple R-squared: 0.8597, Adjusted R-squared: 0.8182 ## F-statistic: 20.69 on 8 and 27 DF, p-value: 1.237e-09 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.8182 train_i1l1 &lt;- train_i1l1 %&gt;% select(-May) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11832.8 -1758.4 -308.5 1480.3 10954.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.475e+05 4.850e+04 -5.102 2.10e-05 *** ## ndate 2.284e+01 2.769e+00 8.249 5.62e-09 *** ## Jan -1.063e+04 3.401e+03 -3.125 0.004114 ** ## Mar 4.727e+03 3.128e+03 1.511 0.141988 ## Oct 7.839e+03 3.010e+03 2.604 0.014572 * ## Nov 5.811e+03 3.106e+03 1.871 0.071872 . ## Dec -1.015e+04 3.092e+03 -3.284 0.002751 ** ## allords_volume 2.181e-06 5.508e-07 3.959 0.000469 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4847 on 28 degrees of freedom ## Multiple R-squared: 0.8468, Adjusted R-squared: 0.8085 ## F-statistic: 22.11 on 7 and 28 DF, p-value: 7.884e-10 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.8085 train_i1l1 &lt;- train_i1l1 %&gt;% select(-Mar) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12675.8 -1882.2 -194.8 3226.3 10807.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.513e+05 4.950e+04 -5.077 2.05e-05 *** ## ndate 2.285e+01 2.830e+00 8.076 6.62e-09 *** ## Jan -1.054e+04 3.475e+03 -3.034 0.005053 ** ## Oct 7.334e+03 3.057e+03 2.399 0.023086 * ## Nov 5.627e+03 3.172e+03 1.774 0.086553 . ## Dec -1.038e+04 3.156e+03 -3.289 0.002638 ** ## allords_volume 2.416e-06 5.399e-07 4.475 0.000109 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4953 on 29 degrees of freedom ## Multiple R-squared: 0.8343, Adjusted R-squared: 0.8 ## F-statistic: 24.34 on 6 and 29 DF, p-value: 4.368e-10 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.8 train_i1l1 &lt;- train_i1l1 %&gt;% select(-Nov) lm_train_i1l1 &lt;- lm(mean_amount ~., data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12894.5 -1933.7 -27.4 3141.3 9781.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.519e+05 5.124e+04 -4.916 2.95e-05 *** ## ndate 2.320e+01 2.922e+00 7.938 7.36e-09 *** ## Jan -1.177e+04 3.526e+03 -3.338 0.002265 ** ## Oct 6.625e+03 3.137e+03 2.112 0.043159 * ## Dec -1.140e+04 3.212e+03 -3.549 0.001297 ** ## allords_volume 2.172e-06 5.405e-07 4.019 0.000363 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5127 on 30 degrees of freedom ## Multiple R-squared: 0.8163, Adjusted R-squared: 0.7857 ## F-statistic: 26.67 on 5 and 30 DF, p-value: 3.389e-10 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) #0.7857 lm_train_i1l1 &lt;- lm(mean_amount ~. + I(ndate^2) + I(ndate^3) + I(ndate^5), data = train_i1l1) summary(lm_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ . + I(ndate^2) + I(ndate^3) + I(ndate^5), ## data = train_i1l1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12075.5 -2036.3 3.2 3322.7 9556.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.795e+09 8.107e+09 0.715 0.48087 ## ndate -1.337e+06 1.873e+06 -0.714 0.48134 ## Jan -1.250e+04 3.778e+03 -3.309 0.00266 ** ## Oct 6.587e+03 3.300e+03 1.996 0.05606 . ## Dec -1.193e+04 3.561e+03 -3.349 0.00240 ** ## allords_volume 2.232e-06 6.057e-07 3.684 0.00101 ** ## I(ndate^2) 1.097e+02 1.538e+02 0.713 0.48180 ## I(ndate^3) -3.374e-03 4.736e-03 -0.713 0.48226 ## I(ndate^5) 1.277e-12 1.795e-12 0.711 0.48318 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5327 on 27 degrees of freedom ## Multiple R-squared: 0.8215, Adjusted R-squared: 0.7686 ## F-statistic: 15.53 on 8 and 27 DF, p-value: 2.828e-08 plot(lm_train_i1l1) lm_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1, train_i1l1), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict, aes(y = mean_amount_pred)) x = model.matrix(~., train_i1l1_ok[,-2]) y = train_i1l1_ok$mean_amount cv.fit_ridge = cv.glmnet(x, y, family = &#39;gaussian&#39;, nfolds = 6, alpha = 1) cv.fit_ridge$lambda.min ## [1] 75.6825 coef(cv.fit_ridge, s = cv.fit_ridge$lambda.1se) ## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -1.450277e+05 ## (Intercept) . ## ndate 1.740200e+01 ## Jan -9.173277e+03 ## Feb . ## Mar . ## May . ## Jul . ## Aug . ## Oct 2.799762e+03 ## Nov . ## Dec -5.893588e+03 ## allords_price . ## allords_volume 1.430776e-06 ridge_predict &lt;- data.frame(ndate = train_i1l1_ok$ndate) ridge_predict$mean_amount_pred &lt;- predict(cv.fit_ridge$glmnet.fit, newx = model.matrix(~., train_i1l1_ok[, -2]), s = cv.fit_ridge$lambda.min) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = ridge_predict, aes(y = mean_amount_pred)) #prediction_ridge = predict(cv.fit_ridge$glmnet.fit, newx = model.matrix(~ ., train_i1l1[, -2]), # type = &quot;class&quot;, #s = cv.fit_ridge$lambda.min) rmse(train_i1l1_ok$mean_amount, ridge_predict$mean_amount_pred) ## [1] 3381.967 rmse(train_i1l1_ok$mean_amount, lm_train_i1l1_predict$mean_amount_pred) ## [1] 4613.697 # over multiple tests lasso consistently 0&#39;s the Feb, Aug, allords_price variables with minimal impact to RMSE error comared to the &quot;ok&quot; model train_i1l1_ok2 &lt;- train_i1l1_ok %&gt;% select(-Feb, -Aug, -allords_price) lm_train_i1l1_ok2 &lt;- lm(mean_amount ~., data = train_i1l1_ok2) summary(lm_train_i1l1_ok2) ## ## Call: ## lm(formula = mean_amount ~ ., data = train_i1l1_ok2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10706.7 -1791.3 82.8 2242.9 7118.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.484e+05 4.589e+04 -5.414 1.13e-05 *** ## ndate 2.282e+01 2.619e+00 8.713 3.44e-09 *** ## Jan -9.193e+03 3.285e+03 -2.798 0.009547 ** ## Mar 6.187e+03 3.027e+03 2.044 0.051183 . ## May 5.416e+03 2.911e+03 1.861 0.074119 . ## Jul 4.753e+03 2.914e+03 1.631 0.114872 ## Oct 9.294e+03 2.917e+03 3.186 0.003726 ** ## Nov 7.258e+03 3.008e+03 2.413 0.023186 * ## Dec -8.703e+03 2.995e+03 -2.906 0.007380 ** ## allords_volume 2.175e-06 5.262e-07 4.134 0.000329 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4584 on 26 degrees of freedom ## Multiple R-squared: 0.8728, Adjusted R-squared: 0.8287 ## F-statistic: 19.82 on 9 and 26 DF, p-value: 1.745e-09 plot(lm_train_i1l1_ok2) lm_train_i1l1_predict_ok2 &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1_ok2, train_i1l1_ok2), ndate = train_i1l1$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lm_train_i1l1_predict_ok2, aes(y = mean_amount_pred)) x = model.matrix(~., train_i1l1_ok2[,-2]) y = train_i1l1_ok2$mean_amount cv.fit_ridge_ok = cv.glmnet(x, y, family = &#39;gaussian&#39;, nfolds = 10, alpha = 1) cv.fit_ridge_ok$lambda.min ## [1] 191.8827 coef(cv.fit_ridge_ok, s = cv.fit_ridge_ok$lambda.1se) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -1.624748e+05 ## (Intercept) . ## ndate 1.833026e+01 ## Jan -9.439266e+03 ## Mar 7.278698e+02 ## May . ## Jul . ## Oct 3.636235e+03 ## Nov 8.952965e+02 ## Dec -6.636628e+03 ## allords_volume 1.557927e-06 ridge_predict_ok &lt;- data.frame(ndate = train_i1l1_ok2$ndate) ridge_predict_ok$mean_amount_pred &lt;- predict(cv.fit_ridge_ok$glmnet.fit, newx = model.matrix(~., train_i1l1_ok2[, -2]), s = cv.fit_ridge_ok$lambda.min) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = ridge_predict_ok, aes(y = mean_amount_pred)) #prediction_ridge = predict(cv.fit_ridge$glmnet.fit, newx = model.matrix(~ ., train_i1l1[, -2]), # type = &quot;class&quot;, #s = cv.fit_ridge$lambda.min) rmse(train_i1l1_ok2$mean_amount, ridge_predict_ok$mean_amount_pred) ## [1] 3939.644 rmse(train_i1l1_ok2$mean_amount, lm_train_i1l1_predict_ok2$mean_amount_pred) ## [1] 3895.29 # drop extra columns from test test_i1l1_mod &lt;- test_i1l1 %&gt;% select(ndate, mean_amount, Jan, Mar, May, Jul, Oct, Nov, Dec, allords_volume) #testset test lm_test_i1l1_pred &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1_ok2, test_i1l1_mod), ndate = test_i1l1_mod$ndate) ridge_predict_test &lt;- data.frame(ndate = test_i1l1_mod$ndate) ridge_predict_test$mean_amount_pred &lt;- predict(cv.fit_ridge_ok$glmnet.fit, newx = model.matrix(~., test_i1l1_mod[, -2]), s = cv.fit_ridge_ok$lambda.min) # baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = ridge_predict, aes(y = mean_amount_pred)) rmse(test_i1l1_mod$mean_amount, ridge_predict_test$mean_amount_pred) ## [1] 8960.708 rmse(test_i1l1_mod$mean_amount, lm_test_i1l1_pred$mean_amount_pred) ## [1] 8672.532 ggplot(test_i1l1_mod, aes(x = ndate, y = mean_amount)) + geom_point() + geom_line() + geom_line(color =&quot;red&quot;, data = ridge_predict_test, aes(y = mean_amount_pred)) + geom_line(color =&quot;blue&quot;, data = lm_test_i1l1_pred, aes(y = mean_amount_pred)) # check agains best model # drop extra columns from test test_i1l1_mod &lt;- test_i1l1 %&gt;% select(-trindex_predict, -asx20_price, -Jun, -Sep) #testset test lm_test_i1l1_pred &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1_high, test_i1l1_mod), ndate = test_i1l1_mod$ndate) # baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = ridge_predict, aes(y = mean_amount_pred)) rmse(test_i1l1_mod$mean_amount, ridge_predict_test$mean_amount_pred) ## [1] 8960.708 rmse(test_i1l1_mod$mean_amount, lm_test_i1l1_pred$mean_amount_pred) ## [1] 11334.3 ggplot(test_i1l1_mod, aes(x = ndate, y = mean_amount)) + geom_point() + geom_line() + geom_line(color =&quot;red&quot;, data = ridge_predict_test, aes(y = mean_amount_pred)) + geom_line(color =&quot;blue&quot;, data = lm_test_i1l1_pred, aes(y = mean_amount_pred)) # the more generalised model produced more accurate results in the test-set than the most accurate training model 6.3 All Ordinaries Volume # As all ordinaries is a predictor the december value needs to be predicted, the increase in data availability will be leveraged here to create a more accurate model, volumes go back to july 2003 allords_full &lt;- read_csv(&quot;./integrate_data/allords_0603.csv&quot;) ## Parsed with column specification: ## cols( ## Date = col_date(format = &quot;&quot;), ## Price = col_number(), ## Open = col_number(), ## High = col_number(), ## Low = col_number(), ## Vol. = col_character(), ## `Change %` = col_character() ## ) names(allords_full) &lt;- c(&quot;date&quot;, &quot;price&quot;,&quot;open&quot;,&quot;high&quot;,&quot;low&quot;,&quot;volume&quot;,&quot;change&quot;) allords_full_date &lt;- allords_full$date allords_full[] &lt;- lapply(allords_full, function(x) gsub(&quot;%&quot;, &quot;e-2&quot;, x)) allords_full[] &lt;- lapply(allords_full, function(x) gsub(&quot;B&quot;, &quot;e9&quot;, x)) allords_full[] &lt;- lapply(allords_full, function(x) as.numeric(x)) ## Warning in FUN(X[[i]], ...): NAs introduced by coercion allords_full$date &lt;- as.numeric(allords_full_date) cor_asx2 &lt;- cor(allords_full) corrplot(cor_asx2, method = &quot;number&quot;) trainrows &lt;- ceiling(0.85*nrow(allords_full)) train_aofull &lt;- allords_full[1:trainrows,] test_aofull &lt;- allords_full[(trainrows+1):nrow(allords_full),] str(train_aofull) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 137 obs. of 7 variables: ## $ date : num 17106 17075 17045 17014 16983 ... ## $ price : num 5502 5402 5525 5529 5644 ... ## $ open : num 5402 5525 5529 5644 5310 ... ## $ high : num 5584 5579 5561 5692 5651 ... ## $ low : num 5139 5347 5294 5510 5238 ... ## $ volume: num 1.94e+10 1.57e+10 1.84e+10 1.78e+10 1.57e+10 ... ## $ change: num 0.0185 -0.0222 -0.0008 -0.0203 0.0628 -0.0252 0.0248 0.0319 0.0412 -0.0215 ... ggplot(train_aofull, aes(x = date, y = volume)) + geom_line() aolm &lt;- lm(volume ~., data = train_aofull) summary(aolm) ## ## Call: ## lm(formula = volume ~ ., data = train_aofull) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.644e+09 -3.300e+09 -7.010e+08 2.414e+09 1.940e+10 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.662e+10 6.448e+09 4.129 6.47e-05 *** ## date 3.529e+05 3.719e+05 0.949 0.3444 ## price -1.386e+07 1.538e+07 -0.901 0.3690 ## open 5.978e+06 1.571e+07 0.380 0.7042 ## high 1.136e+07 6.104e+06 1.862 0.0649 . ## low -6.438e+06 3.801e+06 -1.694 0.0927 . ## change 5.490e+10 7.447e+10 0.737 0.4623 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.138e+09 on 130 degrees of freedom ## Multiple R-squared: 0.1842, Adjusted R-squared: 0.1465 ## F-statistic: 4.892 on 6 and 130 DF, p-value: 0.0001536 x = model.matrix(~., train_aofull[,-5]) y = train_aofull$volume cv.fit_ridge_ok = cv.glmnet(x, y, family = &#39;gaussian&#39;, nfolds = 10, alpha = 0.5) cv.fit_ridge_ok$lambda.min ## [1] 168455586 coef(cv.fit_ridge_ok, s = cv.fit_ridge_ok$lambda.min) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 5.986809e+08 ## (Intercept) . ## date . ## price . ## open . ## high . ## volume 9.700569e-01 ## change . #ridge_predict_ok &lt;- data.frame(ndate = $ndate) ## ridge_predict_ok$mean_amount_pred &lt;- predict(cv.fit_ridge_ok$glmnet.fit, newx = model.matrix(~., train_i1l1_ok2[, -2]), # s = cv.fit_ridge_ok$lambda.min) # Dec 2016 - Volume 16.35B # predict december 2016 december_prediction &lt;- train_i1l1_ok2[1,] december_prediction$ndate &lt;- as.numeric(as.Date(&quot;2016-12-01&quot;)) december_prediction$Jan &lt;- 0 december_prediction$Dec &lt;- 1 december_prediction$allords_volume &lt;- as.numeric(16.35e9) december_prediction$mean_amount &lt;- NA dec_i1l1_pred &lt;- data.frame(mean_amount_pred = predict(lm_train_i1l1_ok2, december_prediction), ndate = december_prediction$ndate) dec_i1l1_pred$mean_amount ## [1] 169433.5 ggplot(test_i1l1_mod, aes(x = ndate, y = mean_amount)) + geom_point() + geom_line() + geom_line(color =&quot;red&quot;, data = ridge_predict_test, aes(y = mean_amount_pred)) + geom_line(color =&quot;blue&quot;, data = lm_test_i1l1_pred, aes(y = mean_amount_pred)) + geom_point(color = &quot;green&quot;, data = dec_i1l1_pred, aes(y= mean_amount_pred)) #169433.5 6.4 Experiment train_i1l1_do &lt;- train_i1l1 %&gt;% mutate(ndatem = ndate*20.83) #poly() did not impreove results either lmdo_train_i1l1 &lt;- lm(mean_amount ~ ndate + I(ndate^4), data = train_i1l1_do) summary(lmdo_train_i1l1) ## ## Call: ## lm(formula = mean_amount ~ ndate + I(ndate^4), data = train_i1l1_do) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17061 -6873 1466 5854 16437 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.448e+05 2.234e+06 0.423 0.675 ## ndate -7.128e+01 1.835e+02 -0.388 0.700 ## I(ndate^4) 5.375e-12 1.071e-11 0.502 0.619 ## ## Residual standard error: 9066 on 33 degrees of freedom ## Multiple R-squared: 0.3682, Adjusted R-squared: 0.3299 ## F-statistic: 9.614 on 2 and 33 DF, p-value: 0.0005129 plot(lmdo_train_i1l1) lmdo_train_i1l1_predict &lt;- data.frame(mean_amount_pred = predict(lmdo_train_i1l1, train_i1l1_do), ndate = train_i1l1_do$ndate) baseplot_train_i1l1 + geom_line(color =&quot;red&quot;, data = lmdo_train_i1l1_predict, aes(y = mean_amount_pred)) "],
["fitting-to-all-groups.html", "7 Fitting to all groups 7.1 Performance 7.2 improvements 7.3 Industry 6 Location 1 (round 2) 7.4 Industry 3 location 9 7.5 dply deployment", " 7 Fitting to all groups # import prepped data load(file = &quot;./core_data/transactions_prepd.Rdata&quot;) load(file =&quot;./core_data/transactions_clean.Rdata&quot;) transactions_prepd &lt;- transactions_prepd %&gt;% select(-sdtrans) #final variable list industry_location &lt;- transactions_prepd %&gt;% select(ndate, location, industry, mean_amount, Jan, Mar, May, Jul, Oct, Nov, Dec, allords_volume) # industry_location is the aggregated data frame output = data.frame() decpred &lt;- data.frame(ndate = as.numeric(as.Date(&quot;2016-12-01&quot;)), mean_amount = NA, Jan = 0, Mar = 0, May = 0, Jul = 0, Oct = 0, Nov = 0, Dec = 1, allords_volume = 16.35e9) december_predict_all &lt;- data.frame() industries = unique(as.numeric(industry_location$industry)) locations = unique(as.numeric(industry_location$location)) for (ind in industries) { for (loc in locations) { # create a subset of the data temp = industry_location[industry_location$industry == ind &amp; industry_location$location == loc, ] # Check to make sure you have at least X months of data if (nrow(temp) &gt;= 36) { # train your model # INSERT YOUR MODEL TRAINING CODE, INCLUDING TRAINING/TEST PARTITIONING trainrows &lt;- ceiling(0.75*nrow(temp)) train_temp &lt;- temp[1:trainrows,] test_temp &lt;- temp[(trainrows+1):nrow(temp),] model &lt;- lm(mean_amount ~., data = select(train_temp, -industry, -location)) # output a prediction train_temp$prediction = predict(model, train_temp) test_temp$prediction = predict(model, test_temp) temp$prediction = predict(model, temp) # CALCULATE YOUR ERROR train_error &lt;- rmse(train_temp$mean_amount, train_temp$prediction) test_error &lt;- rmse(test_temp$mean_amount, test_temp$prediction) total_error &lt;- rmse(temp$mean_amount, temp$prediction) # append your error to the output data frame, include industry and location variables row &lt;- cbind(ind,loc,train_error,test_error, total_error) output = rbind(output, row) #predict decemeber dpred &lt;- data.frame(industry = ind, location = loc, prediction = predict(model, decpred), acc_plu_minus = test_error) december_predict_all &lt;- rbind(december_predict_all, dpred) } } } output ## ind loc train_error test_error total_error ## 1 1 1 3895.290 8672.532 5406.023 ## 2 1 2 7517.271 10268.024 8243.750 ## 3 1 3 5294.130 15360.581 8757.274 ## 4 1 4 7699.804 8483.127 7890.109 ## 5 1 5 5576.327 15919.373 9117.584 ## 6 1 6 5853.082 9696.393 6945.883 ## 7 1 7 14949.199 17754.280 15650.836 ## 8 1 8 4650.881 15813.074 8665.524 ## 9 1 9 8073.405 14045.066 9802.711 ## 10 1 10 8503.050 14888.191 10356.525 ## 11 2 1 21241.467 20906.393 21163.521 ## 12 2 2 16998.259 22143.519 18332.368 ## 13 2 3 39598.871 106479.033 62085.420 ## 14 2 4 36074.533 36848.062 36257.051 ## 15 2 5 38654.379 129285.617 71108.589 ## 16 2 6 21412.246 81695.846 43740.472 ## 17 2 7 19180.581 64594.705 35472.928 ## 18 2 8 43836.984 125068.506 71643.913 ## 19 2 9 50607.327 36573.985 47694.479 ## 20 2 10 21698.464 89477.378 47269.677 ## 21 3 1 77693.401 173253.737 107929.346 ## 22 3 2 42081.707 68963.351 49694.107 ## 23 3 3 82700.509 303411.435 163659.023 ## 24 3 4 64994.910 71311.605 66527.066 ## 25 3 5 132606.388 179201.536 144861.301 ## 26 3 7 49881.767 50451.723 50015.743 ## 27 3 8 33914.699 34886.172 34160.158 ## 28 3 9 148122.385 794161.575 405480.208 ## 29 4 1 21388.094 71688.032 39410.317 ## 30 4 2 6981.621 12508.189 8599.545 ## 31 4 3 14494.126 27516.177 18387.906 ## 32 4 4 21603.664 28716.983 23462.589 ## 33 4 5 25464.219 46839.963 31782.893 ## 34 4 6 138553.013 300892.292 189455.414 ## 35 4 7 18415.599 29821.522 21631.040 ## 36 4 8 12591.545 14662.995 13105.732 ## 37 4 9 11075.150 14283.310 11903.749 ## 38 4 10 21012.606 39372.696 26476.552 ## 39 5 1 59161.965 91496.155 68119.417 ## 40 5 2 29451.600 30488.851 29697.609 ## 41 5 3 49704.337 42248.602 48073.742 ## 42 5 4 42128.382 122087.010 69626.731 ## 43 5 5 15167.065 156243.737 77541.651 ## 44 5 6 61724.744 162117.231 95233.191 ## 45 5 7 29825.216 36721.676 31574.587 ## 46 5 8 38860.540 38781.761 38842.117 ## 47 5 9 45845.884 94178.541 60710.684 ## 48 5 10 76662.653 164356.921 104037.945 ## 49 6 1 3398587.535 11313638.885 6229300.998 ## 50 7 1 18657.862 71535.014 38266.148 ## 51 7 2 19108.429 25538.778 20792.427 ## 52 7 3 84381.100 32052.712 75459.904 ## 53 7 4 45182.621 67755.795 51362.752 ## 54 7 5 43623.216 56127.253 46849.786 ## 55 7 6 34767.971 35224.528 34875.361 ## 56 7 7 13064.627 33449.670 19814.204 ## 57 7 8 34562.677 39695.372 35829.911 ## 58 7 9 87045.702 218993.216 130490.781 ## 59 7 10 49110.600 111821.184 69092.966 ## 60 8 1 232578.881 619268.640 362197.073 ## 61 8 2 49774.260 109477.258 68576.272 ## 62 8 3 107934.532 78985.444 101899.071 ## 63 8 4 33285.550 88185.518 51659.471 ## 64 8 5 176349.354 116664.553 164335.167 ## 65 8 6 114534.722 231100.285 150158.501 ## 66 8 7 165097.213 264819.477 193108.840 ## 67 8 8 39560.720 94849.498 57483.119 ## 68 8 9 26480.756 70389.712 41191.334 ## 69 8 10 99057.465 103150.476 100030.416 ## 70 9 1 85861.232 138191.539 100579.529 ## 71 9 2 59698.637 182168.189 102452.804 ## 72 9 3 87967.438 157814.887 108425.741 ## 73 9 4 476128.292 730155.100 546274.122 ## 74 9 7 92129.789 547239.918 276749.796 ## 75 9 9 68267.270 111490.127 80626.269 ## 76 10 2 25553.898 31339.474 27019.241 ## 77 10 5 6233.101 25480.393 13480.017 ## 78 10 7 102983.701 104309.568 103295.536 ## 79 10 8 757593.976 970693.994 812493.743 grouped_errors &lt;- output 7.1 Performance # evaluating model performance by out-of-set prediction RMSE te_mean &lt;- mean(grouped_errors$test_error) te_sd &lt;- sd(grouped_errors$test_error) te_mean ## [1] 263623.3 te_sd ## [1] 1271581 ggplot(grouped_errors, aes(x = test_error)) + geom_density() #adding an index key grouped_errors$key &lt;- paste(&quot;ind&quot;,grouped_errors$ind,&quot;loc&quot;,grouped_errors$loc, sep = &quot;&quot;) ggplot(grouped_errors, aes(x = key, y = test_error)) + geom_point() #though the labels are not very clear one group has an obviously much poorer performance than all others worst_perf &lt;- grouped_errors[which.max(grouped_errors$test_error),] worst_perf_rem &lt;- grouped_errors[-(which.max(grouped_errors$test_error)),] # unsuprisngly it is industry 6 location 1 wpr_mean &lt;- mean(worst_perf_rem$test_error) wpr_sd &lt;- sd(worst_perf_rem$test_error) wpr_mean ## [1] 121956.5 wpr_sd ## [1] 178426.8 #the mean and distribution are much better with the outlier removed, though as the distribution is quite skewed the SD is not very relevant # seeing the improved plo ggplot(worst_perf_rem, aes(x = test_error)) + geom_density() ggplot(worst_perf_rem, aes(x = key, y = test_error)) + geom_point() + geom_text(aes(label = key), vjust = +1, hjust = &quot;inward&quot;) # second worst performer worst_perf2 &lt;- worst_perf_rem[which.max(worst_perf_rem$test_error),] worst_perf2 ## ind loc train_error test_error total_error key ## 79 10 8 757594 970694 812493.7 ind10loc8 worst_perf2_rem &lt;- worst_perf_rem[-(which.max(worst_perf_rem$test_error)),] # industry 10 location 8, both the industries identified in understanding as having some customers with expetionaly high transaction values # below that ggplot(worst_perf2_rem, aes(x = key, y = test_error)) + geom_point() + geom_text(aes(label = key), vjust = +1, hjust = &quot;inward&quot;) # referring to an earlier plot ggplot(transactions, aes(x= trdate, y = monthly_amount)) + geom_point() + facet_grid(rows = vars(transactions$industry), cols = vars(transactions$location), scales = &quot;free&quot;) ggplot(transactions, aes(x = monthly_amount)) + geom_histogram(bins = 25) + facet_grid(rows = vars(transactions$industry), cols = vars(transactions$location), scales = &quot;free&quot;) # high test_error appears to corrolate with high variation in transaction amount, or when there are sudden changes to transactions e.g. Industry 3 location 9 &amp; industry 9 location 4 # mean by industry mb_ind &lt;- grouped_errors %&gt;% group_by(ind) %&gt;% summarise(mean_terror = mean(test_error), sdtest = sd(test_error)) %&gt;% arrange(desc(mean_terror)) mb_ind ## # A tibble: 10 x 3 ## ind mean_terror sdtest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 11313639. NA ## 2 9 311177. 261239. ## 3 10 282956. 459892. ## 4 3 209455. 252845. ## 5 8 177689. 168490. ## 6 5 93872. 55077. ## 7 2 71307. 41262. ## 8 7 69219. 58710. ## 9 4 58630. 86981. ## 10 1 13090. 3443. # mean by location mb_loc &lt;- grouped_errors %&gt;% group_by(loc) %&gt;% summarise(mean_terror = mean(test_error), sdtest = sd(test_error)) %&gt;% arrange(desc(mean_terror)) mb_loc ## # A tibble: 10 x 3 ## loc mean_terror sdtest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1389850. 3726007. ## 2 9 169264. 261162. ## 3 8 166806. 327120. ## 4 4 144193. 239466. ## 5 6 136788. 114864. ## 6 7 127685. 174693. ## 7 3 95484. 96665. ## 8 5 90720. 62400. ## 9 10 87178. 53545. ## 10 2 54766. 57281. # mean by location without industry 6 mb_loc_n6 &lt;- grouped_errors %&gt;% filter(ind != 6) %&gt;% group_by(loc) %&gt;% summarise(mean_terror = mean(test_error), sdtest = sd(test_error)) %&gt;% arrange(desc(mean_terror)) mb_loc_n6 ## # A tibble: 10 x 3 ## loc mean_terror sdtest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 169264. 261162. ## 2 8 166806. 327120. ## 3 1 149377. 197571. ## 4 4 144193. 239466. ## 5 6 136788. 114864. ## 6 7 127685. 174693. ## 7 3 95484. 96665. ## 8 5 90720. 62400. ## 9 10 87178. 53545. ## 10 2 54766. 57281. # the model performs poorly across groups where there is alot of variation in transaction ammounts 7.2 improvements # would returning the transaction numbers or adding SD to the dataset improve performance for these groups? # SD has been retroactivley added as a variable, but removed until this part of the process #testing on two high error sets 7.3 Industry 6 Location 1 (round 2) load(file = &quot;./core_data/transactions_prepd.Rdata&quot;) industry_location_i6l1 &lt;- transactions_prepd %&gt;% select(ndate, location, industry, mean_amount, Jan, Mar, May, Jul, Oct, Nov, Dec, allords_volume, sdtrans, ntrans) output_i6l1 = data.frame() decpred_i6l1 &lt;- data.frame(ndate = as.numeric(as.Date(&quot;2016-12-01&quot;)), mean_amount = NA, Jan = 0, Mar = 0, May = 0, Jul = 0, Oct = 0, Nov = 0, Dec = 1, allords_volume = 16.35e9) december_predict_i6l1 &lt;- data.frame() i6l1 = industry_location_i6l1[industry_location_i6l1$industry == 6 &amp; industry_location_i6l1$location == 1, ] trainrows &lt;- ceiling(0.75*nrow(i6l1 )) train_i6l1 &lt;- i6l1 [1:trainrows,] test_i6l1 &lt;- i6l1 [(trainrows+1):nrow(i6l1 ),] model_i6l1 &lt;- lm(mean_amount ~., data = select(train_i6l1, -industry, -location)) # output a prediction train_i6l1$prediction = predict(model_i6l1, train_i6l1) test_i6l1$prediction = predict(model_i6l1, test_i6l1) i6l1 $prediction = predict(model_i6l1, i6l1 ) # CALCULATE YOUR ERROR train_error &lt;- rmse(train_i6l1$mean_amount, train_i6l1$prediction) test_error &lt;- rmse(test_i6l1$mean_amount, test_i6l1$prediction) total_error &lt;- rmse(i6l1 $mean_amount, i6l1 $prediction) # append your error to the output data frame, include industry and location variables ind = 6 loc = 1 row &lt;- cbind(ind,loc,train_error,test_error, total_error) output_i6l1 = rbind(output_i6l1, row) # i6l1_orig &lt;- worst_perf i6l1_orig$iteration &lt;- &quot;before&quot; output_i6l1$key &lt;- &quot;ind6loc1&quot; output_i6l1$iteration &lt;- &quot;after&quot; i6l6_compare &lt;- rbind(i6l1_orig, output_i6l1) i6l6_compare ## ind loc train_error test_error total_error key iteration ## 49 6 1 3398588 11313639 6229301 ind6loc1 before ## 1 6 1 2116380 7038670 3876327 ind6loc1 after i6l6_perc_compare &lt;- (i6l6_compare[1,4]-i6l6_compare[2,4])/i6l6_compare[1,4] i6l6_perc_compare ## [1] 0.3778597 #approx 38% decrease in test error summary(model_i6l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = select(train_i6l1, -industry, ## -location)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5061245 -869298 450883 1289373 4031288 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.237e+08 5.535e+07 2.234 0.035069 * ## ndate -6.506e+03 3.246e+03 -2.004 0.056459 . ## Jan -6.552e+05 2.047e+06 -0.320 0.751699 ## Mar -1.895e+06 1.766e+06 -1.073 0.293824 ## May -4.283e+05 1.685e+06 -0.254 0.801482 ## Jul 2.077e+06 1.682e+06 1.235 0.228904 ## Oct 7.676e+05 1.897e+06 0.405 0.689286 ## Nov -4.398e+05 1.833e+06 -0.240 0.812368 ## Dec 9.706e+05 2.445e+06 0.397 0.694885 ## allords_volume 5.059e-04 3.019e-04 1.676 0.106784 ## sdtrans 6.123e-01 1.704e-01 3.593 0.001463 ** ## ntrans -3.871e+06 8.636e+05 -4.482 0.000155 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2592000 on 24 degrees of freedom ## Multiple R-squared: 0.8526, Adjusted R-squared: 0.785 ## F-statistic: 12.62 on 11 and 24 DF, p-value: 1.869e-07 plot(model_i6l1) #the model still shows some residual issues # sd and ntrans would also need to be predicted producing some challenges with this method # dpred &lt;- data.frame(industry = ind, location = loc, prediction = predict(model_i6l1, decpred_i6l1), acc_plu_minus = test_error) # december_predict_i6l1 &lt;- rbind(december_predict_i6l1, dpred) # or by removing the outlier customer load(file =&quot;./core_data/transactions_clean.Rdata&quot;) load(file = &quot;./integrate_data/fin_mets.Rdata&quot;) # this requires rebuilding the aggregated dataset transactions_agg_i6l1 &lt;- transactions %&gt;% filter(customer_id != &quot;6c530aae768250b8d9c3c908a13ee287&quot;) %&gt;% group_by(industry, location, trdate) %&gt;% summarise(mean_amount = mean(monthly_amount) , ntrans = n(), sdtrans = sd(monthly_amount)) %&gt;% select(trdate, everything()) transactions_agg_i6l1$trmonth &lt;- lubridate::month(transactions_agg_i6l1$trdate, label = TRUE, abbr = TRUE) tmonths &lt;- unique(transactions_agg_i6l1$trmonth) for (mnth in tmonths) { transactions_agg_i6l1[transactions_agg_i6l1$trmonth == mnth, mnth] &lt;- 1 transactions_agg_i6l1[transactions_agg_i6l1$trmonth != mnth, mnth] &lt;- 0 } transactions_prepd_i6l1 &lt;- merge(transactions_agg_i6l1, combined_external, by = &quot;trdate&quot;) %&gt;% arrange(industry, location, trdate) # not using the additonal variables for a better comparison with the original industry_location_i6l1_cr &lt;- transactions_prepd_i6l1 %&gt;% select(ndate, location, industry, mean_amount, Jan, Mar, May, Jul, Oct, Nov, Dec, allords_volume) output_i6l1 = data.frame() decpred_i6l1 &lt;- data.frame(ndate = as.numeric(as.Date(&quot;2016-12-01&quot;)), mean_amount = NA, Jan = 0, Mar = 0, May = 0, Jul = 0, Oct = 0, Nov = 0, Dec = 1, allords_volume = 16.35e9) december_predict_i6l1 &lt;- data.frame() i6l1 = industry_location_i6l1_cr[industry_location_i6l1_cr$industry == 6 &amp; industry_location_i6l1_cr$location == 1, ] trainrows &lt;- ceiling(0.75*nrow(i6l1 )) train_i6l1 &lt;- i6l1 [1:trainrows,] test_i6l1 &lt;- i6l1 [(trainrows+1):nrow(i6l1 ),] model_i6l1 &lt;- lm(mean_amount ~., data = select(train_i6l1, -industry, -location)) # output a prediction train_i6l1$prediction = predict(model_i6l1, train_i6l1) test_i6l1$prediction = predict(model_i6l1, test_i6l1) i6l1 $prediction = predict(model_i6l1, i6l1 ) # CALCULATE YOUR ERROR train_error &lt;- rmse(train_i6l1$mean_amount, train_i6l1$prediction) test_error &lt;- rmse(test_i6l1$mean_amount, test_i6l1$prediction) total_error &lt;- rmse(i6l1 $mean_amount, i6l1 $prediction) # append your error to the output data frame, include industry and location variables ind = 6 loc = 1 row &lt;- cbind(ind,loc,train_error,test_error, total_error) output_i6l1 = rbind(output_i6l1, row) # i6l1_orig &lt;- worst_perf i6l1_orig$iteration &lt;- &quot;before&quot; output_i6l1$key &lt;- &quot;ind6loc1&quot; output_i6l1$iteration &lt;- &quot;after&quot; i6l6_compare &lt;- rbind(i6l1_orig, output_i6l1) i6l6_compare ## ind loc train_error test_error total_error key iteration ## 49 6 1 3398588 11313639 6229301 ind6loc1 before ## 1 6 1 5860612 15937391 9260404 ind6loc1 after i6l6_perc_compare &lt;- (i6l6_compare[1,4]-i6l6_compare[2,4])/i6l6_compare[1,4] i6l6_perc_compare ## [1] -0.4086884 #approx 40% increase in test error summary(model_i6l1) ## ## Call: ## lm(formula = mean_amount ~ ., data = select(train_i6l1, -industry, ## -location)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19006125 -4137665 -239879 4980895 8451102 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.801e+08 6.905e+07 -5.505 8.90e-06 *** ## ndate 2.372e+04 3.940e+03 6.021 2.33e-06 *** ## Jan 6.903e+06 4.943e+06 1.397 0.1744 ## Mar 5.854e+05 4.554e+06 0.129 0.8987 ## May 1.527e+06 4.379e+06 0.349 0.7302 ## Jul 6.069e+06 4.384e+06 1.384 0.1780 ## Oct 5.724e+06 4.388e+06 1.304 0.2035 ## Nov 2.951e+06 4.526e+06 0.652 0.5202 ## Dec 1.140e+07 4.506e+06 2.531 0.0178 * ## allords_volume 1.417e-03 7.917e-04 1.790 0.0851 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6896000 on 26 degrees of freedom ## Multiple R-squared: 0.66, Adjusted R-squared: 0.5424 ## F-statistic: 5.609 on 9 and 26 DF, p-value: 0.0002552 plot(model_i6l1) 7.4 Industry 3 location 9 output_i3l9 = data.frame() decpred_i3l9 &lt;- data.frame(ndate = as.numeric(as.Date(&quot;2016-12-01&quot;)), mean_amount = NA, Jan = 0, Mar = 0, May = 0, Jul = 0, Oct = 0, Nov = 0, Dec = 1, allords_volume = 16.35e9) december_predict_i3l9 &lt;- data.frame() #i6l1 aggregated data can be used i3l9 = industry_location_i6l1[industry_location_i6l1$industry == 3 &amp; industry_location_i6l1$location == 9, ] #i3l9 had a slow start, only single transactions per month during part of the first year these NA&#39;s will need to be converted to 0 i3l9[i3l9$ntrans == 1,13] &lt;- 0 trainrows &lt;- ceiling(0.75*nrow(i3l9 )) train_i3l9 &lt;- i3l9 [1:trainrows,] test_i3l9 &lt;- i3l9 [(trainrows+1):nrow(i3l9),] model_i3l9 &lt;- lm(mean_amount ~., data = select(train_i3l9, -industry, -location)) # output a prediction train_i3l9$prediction = predict(model_i3l9, train_i3l9) test_i3l9$prediction = predict(model_i3l9, test_i3l9) i3l9$prediction = predict(model_i3l9, i3l9 ) # CALCULATE YOUR ERROR train_error &lt;- rmse(train_i3l9$mean_amount, train_i3l9$prediction) test_error &lt;- rmse(test_i3l9$mean_amount, test_i3l9$prediction) total_error &lt;- rmse(i3l9 $mean_amount, i3l9$prediction) # append your error to the output data frame, include industry and location variables ind = 3 loc = 9 row &lt;- cbind(ind,loc,train_error,test_error, total_error) output_i3l9 = rbind(output_i3l9, row) # i3l9_orig &lt;- worst_perf i3l9_orig$iteration &lt;- &quot;before&quot; output_i3l9$key &lt;- &quot;ind3loc9&quot; output_i3l9$iteration &lt;- &quot;after&quot; i6l6_compare &lt;- rbind(i3l9_orig, output_i3l9) i6l6_compare ## ind loc train_error test_error total_error key iteration ## 49 6 1 3398587.5 11313639 6229301.0 ind6loc1 before ## 1 3 9 117452.1 1110455 546961.3 ind3loc9 after i6l6_perc_compare &lt;- (i6l6_compare[1,4]-i6l6_compare[2,4])/i6l6_compare[1,4] i6l6_perc_compare ## [1] 0.9018481 #approx 90% decrease in test error! summary(model_i3l9) ## ## Call: ## lm(formula = mean_amount ~ ., data = select(train_i3l9, -industry, ## -location)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -290462 -67198 11862 81166 210404 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.066e+07 5.859e+06 1.820 0.081288 . ## ndate -6.212e+02 3.691e+02 -1.683 0.105376 ## Jan 3.442e+05 1.108e+05 3.107 0.004804 ** ## Mar 7.244e+04 9.561e+04 0.758 0.456010 ## May -2.617e+04 9.330e+04 -0.280 0.781533 ## Jul -6.501e+04 9.463e+04 -0.687 0.498674 ## Oct 5.377e+04 1.002e+05 0.537 0.596421 ## Nov 7.713e+04 9.840e+04 0.784 0.440801 ## Dec 4.577e+05 1.140e+05 4.017 0.000505 *** ## allords_volume 2.752e-06 1.699e-05 0.162 0.872699 ## sdtrans -4.789e-01 1.635e-01 -2.928 0.007356 ** ## ntrans 5.479e+04 4.871e+04 1.125 0.271781 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 143800 on 24 degrees of freedom ## Multiple R-squared: 0.731, Adjusted R-squared: 0.6077 ## F-statistic: 5.928 on 11 and 24 DF, p-value: 0.0001366 plot(model_i3l9) # it appears the additon of particular and SD measure of the aggregated transations would improve the outcome of the model 7.5 dply deployment # beginnings of attempt to use dplyr for deployment on full set #prepd_train &lt;- transactions_prepd %&gt;% # ungroup() %&gt;% #group_by(industry, location) %&gt;% #group_split() # pi1l1 &lt;- transactions_prepd %&gt;% #filter(industry == 1, location == 1) %&gt;% #select(-industry, -location, -trdate) %&gt;% #removing trdate and switching to ndate for ease of calculation #arrange(ndate) %&gt;% #select(ndate, everything()) "]
]
